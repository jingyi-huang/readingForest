{
    "#doc1": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " in this segment I m going to return to dependency parsing right in the first segment I introduced the idea of dependency syntax but let s look again and how that worked so the idea of dependency syntax is you connect up the words of a sentence by putting arrows between them that show relationships of being modifiers or arguments of other words so here in this example we ve got the head of the whole sentence submitted and it s got its dependents so it s got bills submitted by somebody and then also the auxiliary verb were now not necessarily but quite commonly the arrows are typed by the name of some grammatical relation and so we can see that here we ve got the subject of a passive the verbal auxilary and the prepositional relationship the other things that you should know about just a bit of the terminology so firstly when we have an arrow we always have the thing that is the head or the governor here submitted and the thing that is the dependent modifier and FIRREA various words are used as you can see over here here bills so that they re the two ends the governor and the governing of a dependency now I ll be on that there s actually some inconsistency about how things are done so in these slides and in the original dependency grammar work of pigeon ear the arrows run from the governor to the dependent but you can absolutely also find other work that points the arrows the other way and actually if like here you re using height in the tree to show what s dependant of what you actually don t need to have any arrows at all you could just draw these as lines without any arrow head on them okay so as in this example normally what you find is the dependencies form a tree so there s a root node and then for they re everything heads down with words having a single head and in a nice cyclic manner I should mention also there s actually quite common to add sort of one pseudo node at the top often called root or wall which points at the head of the sentence and that actually makes things a lot cleaner both in terms of the parsing algorithms but also in terms of things like evaluation and representation because then you get the property that every word of the sentence including the root is the dependent of one thing and so you can think of it as doing an assignment process of working out what is the governor of each word of the sentence how does the pendency grammar relate to the kind of free structure grammar that we ve concentrated on so far well the syndrome innovation is really that the pendency grammar is built around the notion of having heads and dependents whereas the basic case of a context free grammar there s no notion of a hint whatsoever but actually things are moved on from there if you look at modern X modern linguistic theory that means things like x bar grammar linguists or our modern statistical Parcells when the Charlie at Collins or Stanford parser all of them have a notion of head and use it extensively so for example in all these parsers there s a notion of head rules where to identify some category as the head of a larger category and as soon as you have head rules of the kind that we discussed before well then you can act as straightforwardly get the dependencies out of a free structure representation so basically you kind of have a spine of head chains and then everywhere you have something coming off that that s a dependent so we have a dependency from walk to sue and a dependency from walked into this is another head chain and then we ve got another dependency from in to store head chain and a c store though so we have the basis for dependency representation inside a very structure tree if and only if we have heads represented what about if we go on the opposite direction if you try and go from dependencies to free structure you can reconstruct a free structure tree by taking the closure of the dependencies of a word and saying that those represent a constituent but it slightly changes the representation from what we normally see in free structure trees in particular in a situation like this you can t have a VP node because actually both su and into our dependence of walked and therefore all three of those must have a flat free structure representation where you have the three the head and it s two dependents suing in two how do people go about doing dependency parsing a whole variety of methods have been used for dependency parsing one method to do it is with a dynamic programming algorithm like the CKY algorithm that we saw for free structure parsing now if you do this naively by adding in heads you end up with something similar to the lexicalized probabilistic context free grammars we saw earlier and end up with an O begin to v algorithm but there s a clever reformulation of what the pars items are do JSON eyes in the and which makes the complexity of doing dependency parsing also in cubed which is kind of what you d hope it to be I m just thinking about the nature of the operation but there are a whole bunch of other methods so people have directly used graph algorithms to do dependency parsing so one idea from the algorithms literature is that you can construct a maximum spanning tree for a sentence because since you want all words connected together to be the dependent of something that means you have to build a tree that spans all the words in the sentence and that s the idea that s used in the well known MST parser i m there other ideas of constraint satisfaction where you start off with an int set of edges between all words and then eliminate ones that don t satisfy hard constraints but a final trend in dependency parsing and actually what we re going to focus on here is a way of doing dependency parsing where you ve had left to right through the sentence and make greedy decisions based on machine learning classifiers as to which words to connect to other words as dependents and so the most well known example of this framework is more positive this just partly because it s very different of what we did for our approach to free structure parsing we looked at in depth but also because it s been shown that this kind of method of doing dependency parsing actually works extremely well and work accurately and exceedingly quickly so it s just a good thing to know about as a different point in the space no matter how we do dependency parsing we need some sources of information to let us choose between possible analyses and which words to take as dependents as other of other words so here s a list of the main sources of information people use so the most obvious source of information is by lexical dependencies so if we have something like a dependency between issues and the well we can look at the word that s the head and look at the word of the dependent and say is that likely that s similar to the by lexical dependencies of our earlier lexicalized pcfg s but we don t want to use that as our only source of information partly because lexical information is so sparse and there are several other good sources of information so let s just go through those so one is the distance between the head and the dependent and if you look at this picture what you ll see is that most dependencies are short there with nearby words there are a couple of exceptions so this dependency here is a pretty long one but most of them are pretty short other sources of information are what is the intervening material so in general dependencies don t cross over verbs and commonly they don t cross over punctuation some exceptions commas are quite often crossed over so looking at the words in between can give information about whether a dependency is likely or not a final source of information is looking at the valency of heads and that s saying for a particular word what kind of dependence does it typically take so would like the typically takes no dependence on the left and no dependence on the right as in this case here on the other hand if you have a word that is say a noun it will take dependence like adjectives and articles on the left but it won t take those kind of dependence on the right so it can take other kinds of words as dependence on the right for example take prepositional phrase modifiers or relative clauses as dependent on the right so you can develop a quite rich typology of what kind of dependence words take okay that should give you a better sense of what the pendency representations look like and the big picture of how we go about parsing with them in the next segment we ll introduce a concrete algorithm for dependency parsing ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "17 - 1 - Dependency Parsing Introduction-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=UTnHwzVAIOo"
    },
    "#doc10": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " the next thing I d like to introduce the term frequency weighting which is one of the components of the kind of document scores that are regularly used in information retrieval systems let s go back to where we began with the term document incidence matrix so with this matrix we recorded a number which was either or in each cell of the matrix depending on whether the word occurred in the document if we then think about what the representation of each document is well what we have is a vector it s a binary vector which the dimensionality of the vector is the size of the vocabulary and it s recording these ones or zeros but we don t have to limit ourselves to a binary vector like this an obvious alternative is instead to move to a count vector so now we still have a vector for each document but rather than simply putting ones and zeros in it we re putting in the number of times the word occurs in the document so we ve still got a vector of size the vocabulary but is now a vector in the natural number vector space previously in the boolean retrieval model we were just looking at a set of words that occurred in the document and doing set operations like and or now with this count model we ve moved to the commonly used bag of words model so in the bag of words model we re not considering the ordering of the words in the document but we are considering how many times a word occurs in the document and this word bag is commonly used for an extension to sets which does record how often a word is used so the backwards model has some huge limitations so John is quicker than Mary and Mary is quicker than John have exactly the same vectors there s no differentiation between them and that s obviously has its limitations so in a sense this is a step back earlier on when we introduced positional indices they were able to English these two documents by either proximity or phrase queries and we want to get back to that we ll look later at recovering positional information but for now we re going to develop the bag of words model and how it s used in vector space retrieval models so we have this quantity of the term frequency of a term and a document which is just the number of times that it occurs and so the question then is how can we use that in a retrieval score thinking about a little I hope you can be convinced that bra term frequency is perhaps not what we really want so the idea underlying making use of term frequency is if I m searching for something like squirrels then I should prefer a document that has the word squirrel and at three times over one that just has the word squirrel and at once but on the other hand if I find a document that has word squirrel in at times it s not clear that I should prefer it s times as much as the document that only mentions squirrel once and so the suggestion is that relevance goes up with number of mentions but not linearly and so we want to come up with some way of scaling term frequencies that is relative to its frequency but less than linear before I go on to outline such a measure let me just highlight one last point we talk here about term frequency now the word frequency actually has two usages one is the rate at which something occurs the frequency of burglaries and the other sense of it is the one that s always used in information retrieval so when we talk about frequency in information retrieval frequency just means the count so the count of a word in a document okay so this now is what is standard Lee done with the term frequency what we do is we take the log of the term frequency now if the term frequency is zero the word doesn t occur in the document well the log of zero is negative infinity so that s slightly I m problematic so the standard fix for that is we have this two case construction where we add one to the term frequency if the term does occur in the document so if it occurs once then this value will become one because the log will be zero and then we ll add one to it and we return an answer of zero if the word doesn t occur so that means that if going on a little and if we use base logarithms as here you can see how we re getting this less than linear growth so if a word occurs twice in a document it gets away to one point three a little more if it occurs ten times it gets a weight of two a thousand times away to four and so on so in order to score a document query pair we re just going to sum over these terms for each word in the query and the document so it s sufficient to take the intersection of words they re in both the query in the document because everything else will contribute nothing to the score and then for each of those terms we re going to calculate this quantity and sum them up and so note in particular that the score is indeed still zero of none of the query terms as present in the document okay so that s the idea of term frequency weighting and how it can be used to give a score for documents for a particular query which can be used to rank the documents returned ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 3 - Term Frequency Weighting-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=43WvJU4LaUg"
    },
    "#doc11": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " in this segment I m going to introduce another score that s used for making the matches of documents to a query and that is to make use of this notion of document frequency in particular we always use in Reverse so I was normally referred to as inverse document frequency weighting the idea behind making use of document frequency is that layer terms are more informative than frequent terms so if you remember earlier on we talked about stop words which was you know words like that and and of and so the idea that was that these words were so common so semantically empty that we didn t have to include them in our information retrieval system at all they had no effect on how good a match a document was to a query well that s maybe not quite true but there s some truth in it in particular it seems like in general very common words aren t very determined of the matching of a document in the query where as rare words are more important so consider a term in the query that is very rare in the collection perhaps something like a wreck now centric well if someone typed that word into their query and we can find a document that contains the word a wreck now eccentric it s very likely to be a document that the user would be interested in seeing so you want to give a high weight in our match score for rare terms like a wreck now centric on the other hand frequent terms are less informative than rare terms so consider a term that is frequent in the collection like high increase line which might occur in lots of documents well document containing such a term is more likely to be relevant than a document that doesn t if the query contained one of those terms but it s not such a sure indicator of relevance so if frequent terms we want to give positive weights for a document matching a term in the query but lower weights than for rare terms and so the way we re going to go about doing that is by making use of this notion of document frequency scores so what exactly is that well the document frequency of a term is the number of documents that contain the term so what this means is that we re looking at the entire collection so maybe the collection is a million documents and if ten documents have this word we re saying that the document frequency is ten so that s just counting the number of documents that occurs regardless of the number of times that occurs that s something I ll come back to so document frequency is an inverse measure of informative third of informativeness of the term and we also note that the document frequency has to be sum of all term has to be smaller than the number of documents in the collection so putting that together this gives us the measure of inverse document frequency where we start with the document frequency and use it as the denominator and the numerator in here is the number of documents so for a word that appears in just one document this part will be N and for a word that appears in every document its value will be one so it s some value between one and n and so then what we do after that is we take the log of it and the log is used to dampen the effect of inverse document frequency the idea again is that if you just use the absolute score there d be too strong a factor now in this computation as you can see I ve used log to the base and that s very commonly used but actually it turns out that what we use as the base of the log isn t really important okay let s go through a concrete example where again we re going to suppose that the size of our document collection is million documents so if we take an extremely rare word like Calpurnia which let s say occurs in just one document well then what we re going to be doing is we re going to be taking million the number of documents divided by then taking the log of that which means with log to the base there will be if we take a somewhat more common document word that curves in maybe a hundred documents then we re going to get that the inverse document frequency of that is four and so then we can work on down for progressively more common words and the inverse document frequency or countdown and in particular the for the final case if we assume the word that occurred in every one of our documents well then we ve got a million divided by a million which is one and if we take the log of that which is we get the answer zero so the result we actually get is that a word that occurs in every document does have a weight of zero according to an IDF score and has no effect on the ordering of words and retrieval and that makes sense because if it has if it occurs in every document it has no discriminatory value between documents and gets a weight of zero and so what you can see with these numbers overall though is that this inverse document frequency weighting will give a small multiplier to pay more attention to words that are rarer words rather than very common words another thing to note here is that IDF values aren t things that change the each query that there s precisely one IDF value for each term in a collection and that s going to be the same regardless of what query you re issuing of the collection okay here s a yes no question for you guys does the IDF have an effect on ranking for one term queries like this one the answer is no it doesn t IDF has no effect on one term queries so for a one term query you re going to have one of these terms of in over the document frequency and it will be worked out but it s going to be just a scaling factor which since there s only one IDF value for each term will be applied to every document and therefore it won t which affect the rating in any way you only have an effect from IDF when you have multiple terms in the query so for example if we have the query capricious person well now we re in a situation where capricious as a much rarer word and so IDF will say pay much more attention to documents that contain the word capricious than to documents that contain just the word person in making your retrieval results there s another measure that reflects the frequency of a term and indeed you might have been wondering why we re not using it that other measure is what information retrieval people refer to as the collection frequency of a term so the collection frequency of a term is just the total number of times it appears in the collection counting multiple occurrences so that s the measure that we ve been using in other places it s the measure we are using to build unigram language models or when we re working out spam classifiers or something like that but it s not what s usually used in information retrieval ranking systems and this next example can maybe help explain why so here we have two words insurance and try and I pick those two words because they have virtually identical collection frequency overall they both occur somewhat more than ten thousand times in the collection but let s then look at their document frequency so the word try occurs in eight thousand seven hundred odd documents and that stands in contrast to insurance which occurs and slightly under four thousand documents and so what does that mean what that means is that when try occurs in a document attends to occur only once but the try is widely distributed across documents on the other hand when insurance occurs in a document it tends to occur several times a tends to occur two to three times and so what does that reflect it reflects the fact that there tend to be documents about and surance which then mentions surance several times where they don t really tend to be documents about trying and so what does that mean in terms of coming up with the score for retrieval systems with words matching what it seems to suggest is that what we should be doing is giving higher waiting to instances of the word insurance appearing so if we had some kind of imagine some kind of query like try to buy insurance the most important word to make sure we re finding in our documents to match the query is insurance and probably the second most important where it is buy and try should be coming in third place before the near stop word of and so that s an idea that is being correctly captured by looking at the document frequency but as you can see it s not captured by the collection frequency which would score try insurance equally okay so I hope now you know what document frequency weighting is and why people usually use that as a retrieval raking score rather than collection frequency ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 4 - Inverse Document Frequency Weighting-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=a50Hv_N-yHA"
    },
    "#doc12": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " we ve now introduced two weights for terms and documents to use in our information retrieval making process term frequency and inverse document frequency in this segment we re going to put them together to give the tf idf weight of terms the tf idf weight of a term and the document right here is simply the product of its TF weight scaled with a log term as we discussed before times its inverse document frequency weight this is the best known weighting scheme for terms and information retrieval there s been a lot of research and there are many others but if you only know one it s the one to know note in particular one fine point so this little or here and this tf idf waiting that s a it s not a minus sign that we re taking a product so sometimes people indicate that more explicitly by using a dot or using a multiplication sign so one of the features of tf idf waiting here Phi DF waiting increases with the number of times the term occurs in the document so that the tf idf wait for a query term depends on the document it s not independent of the document and then the tf idf wait for a term also goes up with the rarity of the term in the collection that s from the IDF waiting here so using this to find the ranking of documents for a query what we re doing to work out the score of the query in the document is we re taking the terms that appear in both the query in the document the rest of them have no waiting and then we re working out this tf idf wait for each of those terms and then we re summing them to give the score of the document with respect to the query so what have we done here what we ve done is we ve gradually moved from first binary vectors in the original model of doing boolean information retrieval to count vectors which we used when we just had a none scaled term frequency so now we have weight vectors for a document and hence a weight matrix between terms and documents and that s now what we see here so each document is now being represented by a real valued vector so for example the document Julius Caesar is being represented by this vector so that for each document it s in the vector space of real valued numbers where the dimensionality is the number of different terms in our collection again okay and then when we have a bunch of these vectors for each document in our collection we have a term a term document matrix which is now a real valued matrix well say a little bit more later about what some of the properties of this is but hopefully seeing this kind of term document matrix of real numbers there s enough to see how we can do a ranking of documents according to some query according to these tf idf scores that we ve assigned that each term and each document so that s tf idf weighting one of the most central concepts in information retrieval systems ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 5 - TF-IDF Weighting-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=PhunzHqhKoQ"
    },
    "#doc13": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hi again okay we ve already laid some of the groundwork with notions like term frequency and inverse document frequency in this segment what I want to introduce is properly the retrieval model of the vector space model which is one of the most commonly used models of information retrieval in real systems so we saw in the previous segment how we turn documents into real valued vectors and so we now have a V dimensional vector space where V is the number of words in our vocabulary the terms the words are the axes of the space and documents you can think of as either just points in the space or vectors from the origin pointing out to those points so we now have a very high dimensional space tens of millions of dimensions in a real system and you apply this such as in a web search engine a crucial property of these vectors is that they re very sparse vectors most of the entries are because each individual document only typically has a few hundred or thousand words in it so then if we have this vector space of documents how do we handle querying it when a query comes in and the key idea there is that we treat queries in exactly the same way they re also going to be vectors in the same space and then if we do that we can rank documents according to their proximity to the query in this space so proximity corresponds the similarity of vectors and therefore is roughly the reverse of distance and we re doing this because we want to get away from the you re either in or out boolean model and have a relative score as to how well a document matches a query we go to rank more relevant documents higher than less relevant documents let s try and make that all a bit more precise so how can we form wise proximity in a vector space the first attempt is just to take the distance between two points that is the distance between the end points of their vectors and the standard way to do that in a vector space would be Euclidean distance between the points though it turns out York Lydian distance by itself isn t actually a good idea and that s because Euclidean distance is large for vectors of different lengths let me explain what I mean by that let s suppose here is our vector space well what we re finding is the distance between here and here is large in particular it s larger than either the distance here or the distance there but if we actually think of this in terms of an information retrieval problem and look at what s in our space that seems wrong so in this teeny example the two word axes shown are here for gossip and here for jealous and what our query is this is the query that would come out precisely if your query is gossip and jealous so it has both of those words occurring with equal weight well if we then look at our documents what we find is document seems to have a lot to do with gossiping and nothing to do with jealousy and document has a lot to do with jealousy and nothing to do with gossiping whereas document seems just the kind of document we want to get one that has a lot to do with both gossiping and and jealousy so the terms in the document D are very similar to the ones in queue so we want to be saying that that is actually the most similar document and so this suggests a way to solve this problem and move forward and that is rather than just talking about distance what we want to start looking at is the angle in the vector space so the idea is we can use angle instead of distance so list in particular motivate that once more by considering this thought experiment suppose that we take a document and append it to itself giving us a document D Prime so clearly semantically D and D Prime have the same content they cover the same information but if we re just working in a regular vector space with Euclidean distance the distance between the two documents will be quite large that s because if we had a vector and we had this was the vector for D and the vector for D prime would be twice as long pointed out here and so that we have a quite large distance between these two vectors so we don t want to do that instead what we want to notice is that these two vectors are in a line so the angle between the two vectors is zero corresponding to maximal similarity and so the idea is we re going to rank documents according to that angle between the document and the query and so the following two notions are equivalent making documents in decreasing order of the angle between the query in the document and ranking documents in increasing order of the cosine of the angle between the query in the document and so I ll go through that in a little bit more detail but you ll often hear the phrase cosine similarity and this is what we re introducing here and the secret here is just to notice that cosine is a monotonically decreasing function for angles between the interval and degrees so here s the cosine which you should remember so if the angle is the cosine of it as if it s perpendicular degrees like the cosine is and it can keep on going right up to degrees and the cosine is continuing to the send to minus so essentially all we need to observe here is that cosine is a monotonically decreasing function in the range of to and so therefore cosine score serves as a kind of inverse of angle and well that might still make it seem a reason rather strange thing to use I mean we could have just taken the reciprocal of the angle or the negative of the angle and that would have also turned things around so we ve got to measure of closeness between documents as a similarity measure it turns out that the cosine measure is actually standard because there s actually a very efficient way to evaluate the cosine of the angle between documents using vector arithmetic where we don t actually use any transcendental functions like cosine that would take a long time to compute so the starting point of going through this is getting an idea of the length of a vector and how to normalize the length of a vector so for any vector so if we have a vector X we can work out the length of the vector by summing up each of its components squared and then taking the square root around the outside so that if we have something like a vector that s what we re going to do is take squared squared and then take the add those gives take the square root gives and that s the length of the vector just like in the standard Pythagorean triangle okay so if we then take any vector and divide it by its length we then get a unit length vector which will you can think of as a vector that touches the surface of a unit hypersphere around the origin now if we go back to the example that we had earlier of two documents D and D appended it to itself to give D Prime you can see that these documents if they re both length normalized will go back to exactly the same position and because of that once you length normalized vectors long and short documents have comparable weights so the secret of our cosine measure is that we do this length normalization so here s the cosine similarity between two documents which is the cosine of the angle between the two documents and the way we do that is in the numerator we calculate here a dot product so we re taking the individual components of the vector here component by component and multiplying them and taking their sum but then the way we do that is that we ve then got this denominator which is considering the lengths of the vectors and you can write it like this but actually what it s equivalent to is taking each vector and length normalizing it and then taking the dot product of the whole thing because it s these sort of two parts you can factor apart as you wish and so filters written out in full it s over here that we have the length normalizations on the bottom and then this summed up dot product on the top okay where each of these elements Qi is a tf idf weight of term I in the query and di is the tf idf weight of the term in the document um in particular what we might want to do is actually length normalized our document vectors in advance and length normalize our cosine length normalize our query vector once the query comes in and if we do that this cosine similarity measure is simply the dot product of length normalized vectors and so we re simply just taking this sum here in the vector space where as we discussed before in reality we won t do it over all elements of the vector we ll just do it over the terms in the vector the terms in the vocabulary that are in the intersection of ones that appear in queue and the document so going back to the kind of picture we had before we now again have our vector space which again we re showing with just two axes here to keep it viewable which are now poor and rich and we can take any document vector and we can map it down to unit length by doing this length normalization and when we do that we get all document vectors being vectors that touch the surface of this unit hyper sphere which is just a circle in two dimensions and so then when we want to order documents by similarity to a query we take this query here and we re working out the angle or the cosine of the angle to other to other documents so in particular the cosine will be highest for small angles so we ll be if we order these documents in terms of the cosine of the angle the document will rank first will be d then it ll be d and then it will be d okay let s now go through this concretely with an example so in this example what we have is three novels of Jane Austen s and we re going to represent them in the vector space lengthen or lies and then we re going to work out the cosine similarity between the different novels so in other words in this example there is an actually a query vector we re just working out the similarity between the different novels that are our documents so the starting off point is starting with these term frequency count vectors for the different novels and so what we can see is affection is one of Jane Austen s favorite words that appears frequently every novel the word watering only occurs in Wuthering Heights and then other words like jealous and jealous and gossip occur occasionally and so this is going to be our vocabulary for this example that I give and what we re going to want to do is take these term frequency vectors and turn them into length normalized vectors on the unit now for this example I m just going to use term frequency weighting and we re going to leave out idea of waiting to keep it a bit simpler let s see what happens on the next slide okay so here we ve done log frequency weighting of the kind we saw before so what were the zeros say zero and then we are having mapping down so getting a weighting of three the number of times the defection appears in Sense and Sensibility but these vectors aren t yet of the same length this is clearly the longest of the vectors so the next step is to length normalize them so now here are the lengths normalized vectors for three documents and you can see how this vector has gone much shorter than it was here by scaling it down and the property that we have for each of these vectors for there being lengths normalized is that if you took this quantity squared plus this quantity squared plus this quantity squared you would get one and therefore the square root of that sum would also be one so their length one vectors so given that that length one vectors we can then calculate cosines similarities as simply the dot product between the vectors and let s see what happens when we do that okay so then we have the cosine similarity between sense and sensibility and Pride and Prejudice is taking these pairwise products and summing them together and it gives us a cosine similarity of so they re very similar and then we can do it for the other cases and what we see that a sense and sensibility and watering heights it s zero point seven nine and for the final pair is two I m at zero point six nine and the thing that we might wonder is unwise do we have the cosine similarity of Sense and Sensibility and Pride and Prejudice is higher than that percent and Sensibility and watering Heights and so we can try and look at that so we re going to be comparing this one with the other two and what we can see is that the this part of the weathering Heights vector doesn t help at all in producing similarity with Sense and Sensibility the biggest component in the sense and sensibility vector is this one and so that generates a lot of similarity with Pride and Prejudice which also has that word very promptly represented where that word is less represented in Wuthering Heights and so therefore this dot product here that this term in the dot product is much larger and so we get greater similarity and so you can see there that it s sort of the ratio of occurrence of different words than document has a big effect on measuring overall similarity okay I hope that exam will help to make it more specific and that you now have a good idea of what the vector space model with information retrieval is is the idea that we can make documents for retrieval based on their similarity of angles in a high dimensional vector space ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 6 - The Vector Space Model -Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=ZEkO8QSlynY"
    },
    "#doc14": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " okay let me tell you a little bit more about how tf idf scores and the cosine similarity measure get used together in a ranked ir retrieval system i m not going to get a lot into the details and making these systems practical and efficient at least give you a little bit more of a sense so the first thing that you might have already started to notice is that tf idf weighting isn t really one thing is really a family of measures let s just look at that in a little bit more detail so first of all you have the term frequency and what you do with the term frequency and you could just have the natural term frequency but we suggested that that is usually muted by something like log waiting and that s indeed the most common thing to do but it s not the only method that s been used there have been a bunch of other methods that being suggested for normalizing term frequency then if we move on to the document frequency we can not use document frequency weighting at all we could use this kind of log inverse document frequency weighting which is again extremely common but again there are other things that people have tried out doing so if we put these two things together we have tf idf weighting giving us a vector but well we may we ll want to normalize those vectors in some way to have better similarity computations and so we discussed using the cosine length normalization and it turns out that it has some advantages and some disadvantages so again there are other things people have tried including both of these and other ones that have come up more recently so things like pivoted length normalization so in general we have a kind of a broad menu of choices and so at the beginning here of each column I ve given some letter names to these choices these were choices these were choices they were developed in the context of the smart information retrieval system which is a very famous pioneering information retrieval system that was developed at Cornell by Jerry Sultan who was really the father of a lot of modern information retrieval and so these choices could be given names by giving letters from these so if you are using the system that we ve mainly been talking about they ll be coming out as ltc logarithm logarithmic IDF and cosine weighting and so this kind of weighting it then turns out can be used both for queries and documents differently and so let s go through a little bit how that all comes out so we can have different weightings for queries versus documents and if we follow this smart notation the standard way that they represented things is by these six letters with the dot in the middle where there s the document weighting scheme followed by the query weighting scheme and their various variants but one that was quite standard coming out of the smart work in the s was this one so we ll just mention this one in a little bit more detail if we do the query part of it first what we find out is that so there s log query normalization now this is actually owned only makes a difference if you have long queries which might mention words multiple times if really you have short queries and no word is mentioned more than once that you re just going to be getting a weight of one four words that appear and zero four words that don t appear there s then IDF weighting of the query terms and cosine normalization the treatment for the documents is the same except there s actually no IDF normalization of the documents that s something that you might want to think about for a moment is that a bad idea um there s some reasons to want to do that one of them is well you ve already put in an idea factor for the same words in the query because remember that you re only going to get non zero scores for words that occur in both the query and the document and that there are some advantages in terms of efficiency of compressing indices if you re not putting IDF in there um let s take this waiting scheme and again go through a concrete example so we re just going to be working out the score for precisely one document against one query using this weighting scheme but we ll do it in great depth okay so our document is car insurance auto insurance it s a bit of a fake document that we wanted something short and then the query is best car insurance so if we go to the query first s car insurance these are as raw weights and so then we re going to scale those with Margarethe mix scaling that since each word only code once it stays one we then get the document frequency of each of those words which we map onto an inverse document frequency so the rarer words like insurance and getting the highest waiting there we then multiply this column by this column which ends up looking just like the document frequency score except the word that didn t occur and then we turn that into a unit vector with cosine normalization and so this is our final representation of the query vector we then move to the document so the document has some term frequencies that aren t just so we reduce those with term frequency weightings that they look like that in this case there is no IDF component on the document so the weights go to being exactly the same just coming from term frequency and then we again do cosine normalization which gives us this is our final document vector ok so then to work out the score for this document for this query we re then working out the cosine similarity which is simply the dot product of these two lengths normalized vectors and so that s then this fact that he we re only the bottom two components are nonzero so we add those up and the overall score is so the document is a good match for the query though I mean do remember when you re looking at cosine similarities that because of the fact that the cosine kind of is sort of flat up the top here you know flat descending it means that you tend to get four fairly similar documents that cosine scores are sort of biased fairly high so it s more important to remember the ordering than the precise values okay that shows you how we evaluated that document and then we d evaluate a bunch of other documents and then we d want to rank according to their cosine similarity scores a little exercise that you might like to do based on this example is well if you know what the IDF scores are here and the document frequencies you should actually be able to not work out what is the number of documents as being used as the basis of this example okay now let s go through how we can work out cosine scores in a vector space retrieval system for a document collection this is the rough kind of algorithm that we re going to want to use so what we re going to assume here is that the query is a typical short web like query so we re only going to be thinking of the words as either occurring or not occurring in the query and also we re going to skip one other step then we re not actually going to do any length normalization of the query and part of the reason that is when you have a situation like this late normalization of the query is actually unnecessary because the query vector has some length and for whatever it is the effective length normalization would just be a rescaling that applies to all query document calculations and wouldn t change the final result okay so given that background what do we do so we start off by having a scores array for all documents which we set to zero and so we re going to accumulate in here the score of a document for different query terms and so these scores are often also referred to as accumulators okay and then we re also going to have another array for the lengths of the different documents and so then what we do is go through each term in the query and we say well the query term is actually just going to be and then we fetch the postings list for that query so then for each document in the postings list the term has a frequency in that document and we may then want to scale that by doing something like log waiting or something like that but and to give us our document weight for the term and then we re doing the components of the dot product here and summing them into the scores array so in essence we re kind of the outer iteration here is for each query term and we re working out the components of the cosine score for each query term and accumulating it in this scores array um we haven t actually done any length normalization of the documents either yet so then the next step is to work out the length of each document and then divide these scores by the length of the document so this then does the length normalization for different document sizes so given the assumptions I mentioned at the beginning that the query vector is and we don t need two lengths normalize it we have something that is now ordered the same as a length normalized cosine similarity score for the documents and so then for our ranking what we just wanted to return is the some number K of documents their IDs or her representation of them that has the highest value for scores now if you think a little this isn t quite yet a practical algorithm so that if our document collection is huge we wouldn t actually want to build an array which has a cell for every document see that we might have you know twenty billion documents or something like that and so systems use methods to work out which are likely documents and only have accumulators for those documents and similarly at the end it s not a good way to find the most relevant documents by simply doing a linear scan of this scores array and so they re more efficient data structures to do that but I hope that that s given you a general idea of how we can build cosine similarity scoring into a ranked retrieval engine so to summarize the essence of what we ve covered for vector space retrieval is the following steps that the query is represented as a tf idf vector the document is also weighted as a tf idf the documents also represent as tf idf vector and then to score a pair of a query in a document we re working out cosine similarity scores which we straightforwardly use to rank the documents with and then what we ll do in the first instance is return some top case for example the top ten documents according to this score to the user as their initial results and if they ask for more we can then show them more okay so that s the general idea of how we can start to build a tf idf ranked retrieval system ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 7 - Calculating TF-IDF Cosine Scores-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=E3shpvJUZ84"
    },
    "#doc15": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " in this section I ll tell you a little bit more about evaluating the quality of a search engine there are many measures for the quality of a search engine there are technical ones such as how fast does it index and how fast does it search we can look at things like the expressiveness of the query language whether they re able to express complex informational needs with things like phrase queries negations disjunctions people have other desires like having an uncluttered UI and a system that doesn t cost a lot to use all of these are measures that are measurable that we can quantify them and we can get some kind of score of what is their goodness but in practice all of those measures while important tend to be dominated by another measure of user happiness that is the user happy and using this search engine and speed a response and size of the index is certainly factors but by themselves blindingly fast useless answers won t make a user happy so a huge part of user happiness is are the results return results that they want and so that s the metric of relevance of results to a user s information need I mentioned this right at the beginning but just to reiterate once more when evaluating the IR system that we evaluate with respect to an information need so an information need is translate into a query and that s what the IR system actually runs but relevance is assessed relative to information Li not the query so for example if the information need is the person s looking for information on whether drinking red wine is more effective than white wine for reducing your risk of heart attacks now come up with some query for example might be wine red white heart attack effective and that will be submitted to a search engine and in evaluating the effectiveness of the search engine and returning relevant results we re not asking are the results that the search engine returns documents that simply have those words on the page rather we re saying through these commence address the users information need okay well how can we go about doing that well if the search engine returns a set of results well then what we can do for evaluation is if we start off with three things if we have some benchmark collection of documents that we can use for evaluation and we have some benchmark set of queries which are in some sense representative of information needs of interest and then we ve gathered this third thing which is Assessor judgements of where the particular documents are relevant the particular queries commonly in practice this can t be a verb assembled exhaustively certainly not if the document collection is large but at least for a particular set of documents that return by particular search engines we can get the Assessor to judge whether those documents are relevant to the queries well if we have a results set with just these three things we re in business because we can use exactly the same measures that we looked at previously precision recall on the F measure that combines them and these are suitable good measures for exactly the same reason that there are good measures from when we were talking about things like named entity recognition that normally only a few documents will be relevant to a particular query and so we can measure that much better by looking at these measures of precision and recall but what if we ve now moved on to a search engine that returns ranked results we can t just totally straightforwardly use these measures of precision we call an F measure because the system can return any number of results in fact the number it returns normally depends on how often we keep on clicking asking for more but if we sort of look at any initial subset of the results we can then work out the precision and recall for that subset and then by putting them together we can come up with a precision recall curve let s look at how that works so here are the first ten results for a search engine where if labelled each result is either relevant not relevant according to an Assessors judgment and so then we can take any initial subsequence of these documents and work out a recall on precision so for the first document the system got it right it s a relevant document and let s assume that overall there are ten relevant documents in the collection so it s gotten one out of the ten relevant documents and so it s recall is and well since that document was relevant the system was right on the first answer its precision is one at this point well the next document was not relevant so the recall of the first two documents we re down to here now is and the precision is now another not relevant document so the precision is sorry the recall is still and the precision is now dropped to and if we look at the set of the top four documents we ve now found two of the ten relevant one so recall is zero point two and our precision has gone back up again to zero point five the fifth one is also relevant so now our recall is up to zero point three and our recall is up to three out of five zero point six and we can keep on going down maybe you guys could work out what some of these entries are down here the other measure I want to mention one of the most recently most used recent measures is mean average precision if we look at the rate retrieval results ordered this way to give me a bit more room and so the first document returned is irrelevant the second one is not relevant say the third one is not relevant in a relevant one and another relevant one not relevant relevant relevant let s say those are our top eight results what you re doing for mean average precision for first of all you re working on average precision for one query and so the way you do that is by saying let s work out shown at each point that a relevant document is returned because that s when you re increasing recall so here the precision is one here that there are now four documents of the Precision s are half here there are five documents of the precision is here there are seven documents of which four of them are relevant so that s is around you guys can correct my arithmetic here we now have documents of which are relevant and that s and then what we re doing to work out the mean average precision is we re kind of keeping on calculating those numbers in practice normally they aren t calculated exhaustively but they re calculated up to some point let s say a hundred and then you re calculating an average function of all those numbers and that s then the average precision for one query you then calculate the same average precision for all the other queries in your inch mark query collection and you again take the average of that and that then gives you mean average precision so in particular this is what s referred to as macro averaging it s each query that counts equally in the calculation of mean average precision so this is a good measure that evaluates to some extent precision at different recall levels while still being weighted most to what the precision is like for the top few returned documents and at the level of across different queries it s giving equal weight to different queries which tends to be a useful thing to do because you re always interested in how systems return work on queries of rare terms as well as queries of common terms so this is a pretty good measure to think about using for evaluating information retrieval systems ok um there s even more methods that I could talk about probably a good sense of how about how to go about evaluating the performance of a ranked retrieval system ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 8 - Evaluating Search Engines -Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=ds1OKuB7lDw"
    },
    "#doc16": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hi again let s turn to how to compute whether two words are similar or not and we ll start in this section with methods that use a thesaurus to compute a word similarity so where synonymous was a binary relation two words are either synonymous or they re not we often want a looser definition of similarity or distance that says two words are more similar if they share more features of meaning without requirement Li the absolute synonyms and similarity just like synonymy is a relationship between senses not between words so we don t see that the word bank is similar to the word slope we say that sense one of Bank is similar to the word fund actually sends three of fund but it s actually since two of Bank is similar to slope member we had two senses of Bank so even though technically similarities a property of senses will find ways to compute similarly over words as well either by taking the max similarity between senses or something or various ways now word similarity plays a role in lots of applications you might want to know if two words are similar to grab a set of synonyms or similar words for the query someone asks information retrieval or for the answer in question answering lots of times the translation of a word you ll have to look for similar words to help find a translation essay grading where students write an essay and you need to know what the similar words are to the correct word for the concept and so on this comes up a lot and in all of our applications for word similarity we often distinguish technically word similarity from word relatedness so a similar word is a near synonym but a related word can be related in any way so car and bicycle might be similar maybe they re not synonyms because car and automobile are synonyms car and bicycle are not quite so close so they re similar but car and gasoline clearly related gasoline is something that goes with cars but it s not similar to cars in some way so we re generally here looking for similarity but occasionally some of the algorithms will give you instead words that are related and that might or may not be useful depending upon the application there are two classes of similarity algorithms the based algorithms we ll talk about in this section are words nearby in the hydronium hierarchy or do words have similar glosses will use the hierarchy or the glosses as ways of defining similarity and distributional algorithms that we ll talk about in the next section do words have similar distribution or contexts so the simplest of the thesaurus base similarity algorithms is called path based similarity and give you a little picture here of the wordnet hierarchy and here we say two concepts two senses or since Epps will call them concepts for now are similar if they re near each other in this thesaurus hierarchy Maya near each other we mean have a short path in between them and we ll define path lengths somewhat unusually we ll say a concept has a path length one to themselves and two to their nearest neighbor and so on so the word the concept nickel has a path length to nickel to coin and to dime because it goes one for nickel to coin three to dime and the path length between nickel and coinage is similarly and all the way up to money is to coin to coinage to currency to meme exchange down to money and so on nickels even further from Richter scale goes all the way up to standard and then down to Richter scale so the path length formally is plus the number of edges in the shortest path in the hypernym graph between the sense note C and C now we can use we can turn path length which is a distance metric into sympathy similarity metric simply by taking over the distance so we ll take the path length and invert it and we get a similarity metric and we can turn the sense face metrics empath as a metric of similarity between two senses or concepts c or c and turn it into a metric between words by taking the maximum similarity among pairs of senses so for all senses of word and all senses of word I take the similar between each of those word senses in each of those War two senses and I take the maximum similarity between those pairs and that s the similarity between the words so returning to sense base similarity we ve got our metric now for path based similarity over the path length so between nickel and coin we have a path length of over plus member we re adding to the number of edges so it s or and between funded and budget similarly over or between nickel and currency we have edges so over plus or is between nickel and money now we have a plus or distance over um similarity between coinage and um Richter scale or m OS similarity of over or for that matter between nickel and standard also also over or now there s a problem with this basic path base similarity which is that we assume that every link represented a uniform distance nickel to money somehow seems to us they ought to be closer than nickel standard and that s because nodes that are very high in the hierarchy are very abstract we d like a metric that says that nodes whose only link is going all the way up to the top of the hierarchy those are probably not very similar words and to do that we d like to represent the cost of each edge independently the most common solution to this problem is the use of information content similarity metrics first proposed by Resnick in and these define a concept PFC a probability of a concept C as the probability that a randomly selected word in a corpus is an instance of that concept informally what I mean is there s a distinct random variable that ranges over words associated with each concept so every node in a hierarchy has this random variable and for any preached note for that concept every observed noun is either a member of that concept with probability P of C or not a member of that concept with probability minus P of C so every word is a number of the root node which might be called entity or might be called something else in different versions of word net or your own hierarchy and so that means that the probability of whatever the root node is and the probability of nodes right below the root node are going to be very high and the lower you get in the hierarchy the lower the probability let s see how that works so here s a little piece of a hierarchy we ve got entity here there s actually something above this hierarchy in the top and and then we have entity then we have down to geological formation and then some leaf nodes Hill Ridge grotto Coast and so on and we re going to train information content similarity first by training a probability P of C and every instance of a word like Hill counts toward the frequency of all of its parents of itself obviously but also natural elevation geological formation all the way up to entity so if we define the concept words of C words have C is the set of all words that are children of node C I should say plus C itself as well so the words of natural elevation are Hill Ridge and natural elevation itself and the words of geological formation are Hill Ridge grotto coast Shore cave natural elevation and geological formation itself and now we can take for any concept we sum over all the words of that concept so itself and all of its children some the counts of all those words and then normalize by the total number of words in the corpus and that tells us the probability of the concept so the probability that a random word will be an instance of that concept once we ve computed these probabilities we can associate them with a hierarchy so here s probabilities computed by de Cong Lin and so now we can say that the concept coast has probability point while the further up we go in the hierarchy up to entity we have a probability and whatever in this particular version of word debt was above entity will have probability of now that we have probability we just need two more things we ll define the information content of a concept as the negative log probability of that concept so we re just following the information theoretic definition of information there and we ll define the lowest common sub sumer of a node as the lowest note in the hierarchy that subsumes both of them very naturally and so the lowest common sub sumer of hill and coast think about that geological formation the lowest common sub sumer of coast and shore shore now how are we going to use this information content as a similarity metric there are a number of methods the Resnick method we say that the similarity between two words is related to how much information they have in common the more they have in common the more similar they are for Reznick we just say what s in common between two words is the information content of their lowest common sub sumer if I have two concepts what s in common between them is the thing that they share is their inherited thing in common so if I just measure the amount of information in that that is in fact what they have in common so the negative log probability of that least common sub sumer that s their similarity so we ll define that metric the Reznick similarity metric this way an alternative metric for dealing with information theoretic similarity is the Lin metric as with Resnick the more two things have in common the more similar they are but now the new intuition the more differences between a and B the less similar they are we measure commonality by introducing a predicate common which is a proposition stating the commonalities between a and B and I see the information out of information contained in that proposition and to measure the difference between a and B we say that the total description of a and B the sum of everything we know about them is the sum of the commonalities plus the differences so to get the differences we can take the description and subtract out the commonalities so roughly speaking a and B are more similar if I see of common is high and I see AB description is low so the Lin similarity between two concepts a and B is higher when they have more and less when there s a lot of other things about them they don t have in common and Lynne modifies Resnick in defining the information content of the commonality of the two as twice the information of their lowest common sub Sumer and so given two concepts in a hierarchy the Lin similarity is two times the log probability of their least common sub Sumer over the sum of the log probabilities of the two concepts the total of the description that we know about the two concepts let s look at an example in our small sample hierarchy we want to know the Lin similarity between the concepts Hill and Coast and we look at their least lowest common sub Sumer geological formation we take twice the log probability of that divided by the sum of the log probabilities of the two items hill and coast and that gives us for the Lin similarity between hill and coast as now a final thesaurus based similarity metric is called the les calgary them after michael ESCO invented it it s often called lask or extended Lesk and this method instead of using the hierarchy looks at the glasses of the words in the dictionary or thesaurus and the intuition is that two concepts are similar if they re glosses contain similar words so the two word net words drawing paper and decal have lots of similar words paper that especially prepared for use in drafting transferring designs from a specially prepared paper a blah blah blah and we have here the words specially prepared are in common and paper in both definitions and so for each N word phrase that s in both glosses the less Cal government adds a score of N squared so paper is in both glosses that s a length one so we ll add a score of one especially prepared as of length two so we ll add a score of two square root of four so this the total Lesk similarity between drawing paper and decal is five and in fact with most versions of less similarity we don t just look at the glasses of the two words we look at the glasses of the words they re hyper names they re high poems and so when we add all that up so it s a sum over all the words or sometimes of Max over all the words of their similarity so in summary we ve seen three classes of the solar space similarity path length similarity where we two words are similar if there s a short path between them in the hierarchy information theoretic similarity we ve seen two methods Resnick and Lin and there s a third one called Jong Conrad so these are the information theoretic similarity where we re looking at the least common sub similar of a node measuring its probability turning that into a into an information measure and we ve seen less similarity we re given two concepts we take the gloss of them and compute the overlap in words with this kind of waiting that we talked about or we might just look at the glosses not just of the words but of some words in relation to those concepts like their high panama s or high poems and so on but we sum all that up over a specified set of relations and that gives us the less similarity or extended less similarity there are lots of libraries for computing these various thesaurus base methods in Python and ltk has methods and there are Python based tools like word net similarity and there s even a nice web based interface so that you can check out the similarity between two words based on different methods and you can go take a look at all of these we evaluate similarity like many other NLP algorithms in two different ways we can do intrinsic evaluation where we look at the metric itself and say how similar is the the numbers this metric gives to what humans would give on some similar tasks so I get a similarity metric for two words and then I get humans to give me a number how similar are these two words and I compare those so that s intrinsic evaluation more more functional is entrance extrinsic or task based end to end evaluations where I have some application I put my similarity metric in the application and I see how well it improves the application and that application could be word sense disambiguation or spelling error correction or essay grading a common simple extrinsic test that s used is taking the test of English as a foreign language or TOEFL multiple choice vocabulary tests so here we have questions like levied is closest in meaning to which of these four words and we can simply take our similarity metric compute the similarity between levied and imposed levied and believed levied and requested levied and correlated and see if our metric returns the right answer the most similar word where the correct answer is imposed so thesaurus based methods forward similarity are a useful way of telling if two words are similar they re very functional in languages like English where we have lots of thesauruses either for general text in word net or for medical text in mesh there they will work less well when we re working in particular genres where we might not have the right information in the thesaurus or in languages for which the source sources are not as available and so for those applications will turn to the distributional methods that we ll see next ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "20 - 3 - Word Similarity and Thesaurus Methods -NLP-Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=c9zcE1bQhm8"
    },
    "#doc17": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " all right let s turn to the second important method for computing word similarity distributional similarity now thus or I have problems for computing similarity we don t always have a thesaurus for a particular language and even though when we do the so I have problem with a recall so words are missing phrases are missing connections between senses may be missing and in general the soy don t work as well for verbs or adjectives which have less structured type on main relations so for all these reasons we often use distributional models of meaning often called vector space models of meaning and these tend to give you a higher recall than hand built the solar I although they might have lower precision the intuition of these distributional models of meaning comes from early linguistic work so for example zellig harris in said oculist and i dr occur in almost the same environments so if a and b have almost identical environments we say they re synonyms and Firth in back in said you shall know a word by the company it keeps so here s an example um we have a bunch of sentences about test we know neither says a bottle of test we knows on the table everybody likes test we know test we know makes you drunk we make test we know out of corn so from these context words for a human it s very easy to guess that has Glee no means some kind of alcoholic beverage some kind of beer made out of corn so the intuition for an algorithm is two words are similar if they re just surrounded by similar words very simple idea let s see how to make that work remember the term document matrix we saw an information retrieval so each cell and the term document matrix was the count of a term T in a document d so we call it the term frequency of T in d and we thought about it what meant to be a document was to be a count vector so a column in this term document matrix so the document as you like it the Shakespeare play is a count vector over lots of words I ve shown you just four words battle soldier fool clam clown so as you like it is a count vector so two documents are similar if they re vectors were similar so Julius Caesar hi counts for battle and soldier lo counts for fool and clown similarly Henry v hi counts for battle and soldier lo counts for fool and clown so we so we saw that Julius Caesar and Henry v that they were to query in a document or two documents they were similar by this vector similarity men method and we re just going to use the same exact intuition for deciding if two words are similar so look at the words in a term document matrix and now a word is a count vector and two words are similar if they re vectors are similar so fool has high counts in the document as you like it in the document Twelfth Night and low counts in Julius Caesar and Henry the fifth clown has high counts and as you like it in legs and low counts in Julius Caesar and Henry the fifth so we say that fool and clown are similar but probably battle and fool are not similar because battle has high counts here and low counts here but fool has the opposite it has high counts here and low counts here so the intuition of words similarity distributional word similarity is instead of using entire documents like we used for information retrieval let s use smaller contexts we could use a paragraph or we could use just a window of words and now we define a word by a vector over these context counts for whatever this context is so what s suppose we use context of ORS to the left and words to the right here s sample example I ve grabbed from the brown corpus so here s some words the word apricot the word pineapple the word digital and the word information and I ve shown you for each of them just one set of context words before apricot words after apricot from one of the uses of apricot in one one document in the corpus and here s words before and after pineapple and so on so from the various documents examples I d grabbed from the brown corpus some examples for each of these words looking like these examples I can compute little counts and I can build myself the term context matrix so here s the term context matrix for these four words apricot pineapple digital information and they don t ever appear with the word aardvark but the words digital information have the word computer within the ten words of them twice for digital ones for information or the word pinch occurs with apricot and pineapple in the word sugar occurs with them whereas the word data and the word result tends to occur with the word digital in the word information now again we say two words are similar in meaning if their context vectors are similar so apricot has a one for pinch and a for sugar pineapple also has a for pinch that one for sugar and for computer and data so that tells us that probably these words are similar whereas digital and information they re counts occur in other words like computer data and result so they re probably similar as well simple intuition just like what we saw in information retrieval for comparing documents but now we re comparing words and using a reduced context now for the term document matrix for information retrieval we use tf idf weighting we didn t use raw accounts use various kinds of waiting sometimes TF sometimes IDF sometimes both now for the term context matrix it s very common to use a version of point wise mutual information called positive point wise mutual information so let s look at that point wise mutual information is an information theoretic method that says do events x and y occur more often than if they were independent so the point wise mutual information between two things x and y is the probability of the two occurring together divided by the probability of the two the product of the probability of the two independently and we take the log of that so you can see that if the two things occur more often than that you d expect by chance more often and expect by Independence than the numerator will be much higher than the denominator so between two words the point raised mutual information is the log of the probability of the two words occurring together times the product of the two words occurring independently and positive PMI simply replaces all the negative values with zero so um imagine that we have a matrix F R term context matrix we ll call it f for frequency and we ve got words labeled by rows labeled by words and we have columns labeled by contexts which could be a context word so we saw for example a context word aardvark or computer or data or pinch or so on and we re gonna take our here s our counts in each of these each of these counts is the freak is f sub IJ the frequency of row I column J so we re gonna turn those into probabilities first so we ll say that the probability of a word I and J occurring together the joint probability of a word I and a context J is the frequency which with they appear normalized by the sum of all the frequency of all words in all context we sum over the entire matrix that s the the denominator pen the probability of a word that s a row I star is the count of all all of the all of the context that that word occur in so we sum over all possible contexts and we sum all accounts for that word in those context normalized by N and the probability of a context them is the sum over all words of that a context occurs in the account those counts again normalized and we take these probabilities the probability of a word in a context occurring together times the probability of a work over the probability of a word times the probability of a context we take that log and that s our PMI and our positive PMI is if it s less than zero we replace it with zero so let s see how that works in practice we ve got our a little I made a simpler little matrix of counts here for working through our example so the word digital occurs in the context computer twice in the context of data once and the contexts result ones and for Pinscher sugar and we saw before and again our probability of the joint probability of a word and a context is the frequency with which the word occurs in the context normalized by the total n the sum over of all accounts for all words in all contexts let s first compute n n is then the sum of all these things so n is that are our denominators are going to be for all of our various probabilities so now the probability the joint probability of the word event information in the context data so word is information in the context of data we ve got our F sub IJ is and our n is so we have or so that s the joint probability of information and data now we need to compute the probabilities of words and the probabilities of contexts so the probability of word we just sum a row over all context that that word it can occur in so the word information occurs times once in this context plus plus so we have a total of times um over again and of or so that s the probability of a word and we do the same thing for contexts we sum over all words of that context occurs with over those counts and normalize so for the context data we have a plus a so we have over or so if we do this computation for all of our examples we ll get this little matrix here we have the in here we have the joint probabilities all the joint probabilities here we have the marginals the probability of the words and the probability of the context so again here s our and that s the probability of data and here s our that s the probability of the word information and there s our okay now we re ready to turn those probabilities and compute PMI so remember that PMI is the log of the joint over the two marginals so it s going to be our log of over times eight and that s going to be and if we compute that for in every place and we re there negative simply replace them with zero so there s the PMI for information and data and these were all negative numbers that we can replace with zero and we can see that the positive numbers the computer occurs with digital the information occurs with data and results pineapple occurs with pinch and sugar and so on and we have positive numbers for each of these kind of relations it turns out that PMI is biased toward infrequent events and there are various weighting schemes that can help alleviate this so turning in Pentel have a lovely survey paper where they look through these kind of weighting schemes but it turns out that even simple things like add ones smoothing could help alleviate the infrequent event bias of PMI let s look at how that works so here s I ve just done add one smoothing to our accounts this is the account matrix we had a few slides ago but I just added once all the zeros became ones and now here is our table of joint probabilities Award in context with add one smoothing and our marginals again P of context and P of W so if you look at the PM is for our regular accounts and now I ve shown you here actually add two counts turned into P into P PMI values you can see that in the in the regular PMI we have very large values for a cotton pineapple occurring with sugar and pinch even though the counts I don t if you remember only one for those things they re not things that we had a lot of evidence for whereas information and data occurred together very often six times and four times for informational result so it was a problem that the PPI was very high for things that we had very little evidence for now after add to smoothing you can see what it s done is it s smooth you know the affected the low counts more than the high counts and so even though we still see apricot and pineapple correctly being associated with pinch and sugar that now things like data that we saw more often or digital and computer that we saw more often are a little bit more and reasonably weighted with respect to the low count events so ad smoothing can help with this low frequency problem so that s the first half of our introduction to distributional similarity and we ll turn in the second half to how to use cosine metrics to actually compute the similarity ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "20 - 4 - Word Similarity_ Distributional Similarity I --NLP-Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=tYw3gJMumg0"
    },
    "#doc18": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " now there are some kinds of advanced questions that don t tend to be answered in modern commercial systems but are part of modern research systems and let s turn to those now imagine the following harder question what is water spinach and the answer we would like to give is really a full paragraph that talks about water spinach and its history and names for in different languages and so on so we like be able to build that kind of question not by just looking it up on the web but by merging together possible snippets we might get from different sources or we might get hard medical questions like in children with an acute illness what s the efficacy of a particular medicine and an answer there might be again and a summary of things we might see from a particular pubmed paper and we might want to read a sentence and decide and read a document and extract sentences and we can say where they came from and how much we believe them so these are the kind of harder things that are important in the research literature but haven t yet made it into commercial modern commercial systems and for answering these harder questions we often use an approach that s called query focused summarization and query focused summarization means that we re applying a natural language summarization algorithm we re going to summarize multiple documents pull information from multiple documents but it s query focused it s not just a summary of everything it s in those documents it s the parts that are relevant for a query and there are two kinds of algorithms for query focus summarization one of them what we might call the bottom up snippet method we find a set of relevant documents extract information from them bottom up using tf idf deciding how many sentences are relevant by their type tf idf score other kinds of scores and then we combine and modify the sentences into an answer and what I m going to talk about today is a second method which we might can give as an information extraction method and here we build specific answers that work for different question types so we might have definition questions or biography questions or certain medical questions and for each of those we might decide what constitutes good definition question or a good biography question or a good medical question and we re going to build specific answers for those so for example we know that a good biography contains a person s birth and death dates it contains how famous they are what they re famous for their education their nationality and these kind of things whereas a good definition contains what s often called a genus or high pernem so we know what is what is a hodge it s a type of ritual and a good medical answer contains the problem that the medicine is designed to to to solve the intervention what is this drug or intervention we re going to use and then the outcome the result of the study describing this problem so for these three kinds of answers we might know that we need to extract for a definition question we ve got to build a genus detector and a species extractor subtype extractor whereas for a biography a question when you build a date extractor and a nationality extractor well for a drug efficacy question we have to extract the population of study was run on or what the problem or interventional and so on and I ve given you examples here of the kind of sentences we need to extract for these kind of answers so here s a sample architecture for a complex question answering system from Blair goldenson at all here you might have a question like what is the Hajj and we might specify that we d like to search documents and extract an answer of about eight sentences so we ll pull do document retrieval and pull lots of relevant documents and now we re going to build classifiers whose job is to pull out genus species sentences so the Hajj is a pilgrimage the Hajj is a milestone the Hajj is a pillar and so on and a classifier that pulls out other non non kinds of definitional sentences and we re going to cluster these and order these and produce as the response that answer of about eight sentences that summarizes a good response to the question so advanced question in turn algorithms used in the laboratory either use information extraction methods or use bottom up clustering methods to combine information from lots of documents to create a set of sentences that answer a question but more commonly commercial systems are based on factoid answers so either using knowledge bases are more often using information retrieval techniques to find sentences that contain an answer to the question extracting and ranking the answer and then presenting it to the user ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "21 - 5 - Advanced_ Answering Complex Questions-NLP-Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=WRomzf3iwHk"
    },
    "#doc19": {
        "description": "MIT 6.046J Design and Analysis of Algorithms, Spring 2015 View the complete course: http://ocw.mit.edu/6-046JS15 Instructor: Srinivas Devadas  In this lecture, Professor Devadas covers the basics of cryptography, including desirable properties of cryptographic functions, and their applications to security.  License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu",
        "subtitles": "The following content is provided under a Creative Commons license Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free To make a donation or view additional materials from hundreds of MIT courses visit MIT OpenCourseWare at ocw mit edu SRINIVAS DEVADAS All right let s get started Good morning everyone I see a lot of tired faces I m not tired Why are you tired [LAUGHTER I only lecture half the time You guys take the class all the time So today s lecture is about hash functions And you may think that you know a lot about hash functions and you probably do But what we re going to do today is talk about really a completely different application of hash functions and a new set of properties that we re going to require of hash functions that I ll elaborate on And we re going to see a bunch of different applications to things like password protection checking the integrity of files auctions and so on and so forth So a little bit of a different lecture Both today and on Thursday I m going to be going to be doing cryptography and applications not too much of algorithms But we will do a little bit of analysis with respect to whether properties are satisfied in this case by hash functions or not So let s just dive right in You all know what hash functions are There s no real change in the definition But the kinds of hash functions that we re going to be looking at today are quite different from the simple hash functions like taking a mod with a prime number that we ve looked at in the past And the notion of collisions is going to come up again except that again we re going to raise the stakes a little bit So a hash function maps arbitrary strings let me do this right So you re not making a statement about the length of the string You will break it up even if you had a string of length or maybe it was you do want to get a number out of it In a specific range there s going to be a number of bits associated with our hash functions And previously we had a number of slots associated with the output of the hash function But the input could be arbitrary And these arbitrary strings of data are going to get mapped as I just said to a fixed length output And we re going to think about this fixed length as being a number of bits today as opposed to slots in the hash table Because we really aren t going to be storing a dictionary or a hash table in the applications we re going to look at today It s simply a question of computing a hash And because the fixed length output is going to be something on the order of bits or bits there s no way that you could store two arrays to elements in a hash table or even two arrays to really And so we re going to just assume that we re computing these hashes and using them for certain applications I just wrote output twice I guess So map it to a fixed length output We want to do this in a deterministic fashion So once we ve computed the hash of a particular arbitrary string that is given to us we want to be able to repeat that process to get the same hash every time We want to do this in a public fashion So everything is public There s no secrecy There s keyed hash functions that we won t actually look at today but maybe in passing I ll mention it next time We re not looking at keyed hash functions here There s no secrets in any of the descriptions of algorithms or techniques I m going to be describing today And we want this to be random We want it to look random True randomness is going to be impossible to achieve given our other constraints But we re going to try and approximate it with pseudo randomness But we d want it to look random because we are interested as we were in the case of dictionaries and the regular application of hash functions we are interested in minimizing collisions And in fact we re going to raise the stakes really high with respect to collisions We want it to be impossible for you or anyone else to discover collisions And that s going to be an important property of collision resistance that obviously is going to require randomness And those are the three things we want deterministic public and random And so just from a function description standpoint you have star here which implies that it s an arbitrary length strength And we want to go to d And this is a string of length d So that means that you re getting d bits out from your hash function And here the length is greater than or equal to So that s it Not a lot that s new here But a few things that are going to be a little bit different And there s some subtleties here that we ll get to I want to emphasize two things one of which I just said There s no secrecy no secret keys here in the hash functions that we are describing All operations are public So just like you had your hash function which was k mod p and p was a prime and p was public and known to everyone who used the dictionary everything here we are going to be talking about is public So anyone can compute h And we re going to assume that this is poly time computation not too surprising but I m being quite flexible here When you look at dictionaries and you think about using dictionaries and using it to implement efficient algorithms what is the assumption we kind of implicitly made are perhaps explicitly in some cases with respect to computing the hash Anybody Yeah AUDIENCE Constant time SRINIVAS DEVADAS Constant time We assumed so this is not necessarily order right So that s important So we re going to I want to make sure you re watching So you re going to raise the stakes even with respect to the complexity of the hash And as you ll see because of the desirable properties we re going to have to do that We re going to ask for really a lot with respect to these hash functions Nobody can find a collision right And if you have something as simple as k mod p it s going to be trivial to find a collision And so these order hash functions that you re familiar with aren t going to make the grade with respect to any of the properties that we ll discuss in a few minutes All right so remember this is poly time computation And there s lots of examples of these hash functions And for those of you who are kind of into computer security and cryptography already you might have heard of examples like MD and MD These are versions MD stands for message digest These were functions that were invented by Professor Rivest And they had d equals way back when if I recall when they were proposed And these algorithms have since been broken in the sense that it was conjectured that they had particular properties of collision resistance that it would take exponential time for anybody to find collisions And it still kind of takes exponential time but raised to is exponential at one level but constant in another level So you can kind of do it in a few seconds now So a little bit of history I m not going to spend a lot of time on this MD was used to create what was called a secure hash algorithm This is bits And this is not quite broken at this point But that people consider it broken or soon to be broken Right now the recommended algorithm is called SHA secure hash algorithm version three And there was a contest that ran for like months or maybe even longer that eventually was won by what turned into the SHA And they had a different name for it that I can t recall But it turned into SHA And what happened along the way as we went from MD MD SHA to SHA is that this amount of computation that you had to do increased And the complexity of operations that you had to do in order to compute the hash of an arbitrary string increased to the point where you want to think about this as rounds of computation And certainly order d computation where d is the number of bits And perhaps even more So it s definitely not order So as I said a little bit of context with respect to the things that are out there At the end of the lecture I ll give you a sense for how these hash functions are built We re not going to spend a lot of time on creating these hash functions It s really a research topic onto itself and not really in the slope of What is in the scope of and what I think is more interesting which is what we ll focus our energy and time on is the properties of these hash functions And why these properties are useful in a bunch of different apps And so what is it that we want We want a random oracle We want to essentially build something that looks like that deterministic public random And we re going to claim that what we want is this random oracle which has all of these wonderful properties that I m going to describe I m going to describe the random oracle to you and then I m going to tell you about what the properties are And then unfortunately this is an ideal world and we can t build this in the real world And so we re going to have to approximate it And that s where the MD s and the MD s and the SHA s came in OK So this is not achievable in practice So what is this oracle This oracle is on input x belonging to star So that could be an arbitrary string If x not in the book so there s this the book all right And there s this infinite capacity book that has all of the computations that were ever done prior And they re always stored in the book And that s how we re going to get determinism Because this book initially gets filled in All of the entries in the book are filled in using pure randomness So you flip a coin d times to determine h of x So that s basically it And you just keep flipping You have to flip d times And so if x was you flip d times d was You flipped a coin times and got a string If x were flip times you get a different string with very high probability obviously And so on and so forth But what you do is you have this book So you re going to record x h of x in the book OK So at some level your hash function is this giant look up table in the sky right Actually not giant infinite capacity look up table in the sky Because you can put arbitrary strings into this And if it s in the book this is obviously the important part that gives you determinism then you return y where x and y are in the book OK So you get a random answer every time except as required for consistency with previous answers So the very first time you see a string or and the whole world can create this book It s public So if I created the book at first with a particular string let s say Eric I was the string And I m the one who put the entry x equals Eric and h of x h of Eric equals some random bit string into the book I get credit for that right But if you come a nanosecond later and ask for h of Eric you should get exactly what got put into the book when I asked for h of Eric And so on and so forth So this is true for everybody So this is like I mean basically impossible to get Because not only can anybody and everybody query you have to have this ordering associated with people querying the book And you have to have consistency All right So everyone convinced that we can t build this All right If you took anything out of this lecture that s what you should take No no There s a lot more So we want to approximate the random oracle And we re going to get to that Obviously we re going to have to do this in poly space as well So what s wrong with this Of course this picture is I didn t actually say this but you d like things to be poly time in terms of space You don t want to store an infinite number this is worse than poly time worse than exponential time because it s arbitrary strings that we re talking about here right So you can t possibly do that So we have to do something better But before I get into how we d actually build this and give you a sense of how SHA and MD were built and that s going to come a little bit later I want to spend a lot of time on the what is interesting which are the desirable properties Which you can kind of see using the random oracle So what is cool about the random oracle is that it s a simple algorithm You can understand it You can t implement it But now you can see what wonderful properties it gives you And these properties are going to be important for our applications OK And so let s get started with a bunch of different properties And these are all properties that are going to be useful for verification or computer security applications The first one it s not ow it s O W It s one wayness all right So one way or one wayness And it s also called you re not going to call it this but perhaps this is a more technical term a more precise term pre image resistance And so what does this mean Well this is a very strong requirement I mean a couple of other ones are also going to be perhaps stronger But this is a pretty strong requirement which says it s infeasible given y which is in the it s basically a d bit vector to find any x such that h of x equals y And so this is x is the pre image of y So what does this say It says that I want to create a hash function such that if I give you a specific we call it a bit string because we re talking SHA here and that s the hash I m going to have it s going to have to be impossible for me to discover an x that produced that bit string OK Now if you go look at our random oracle you realize that if you had a bit string and perhaps you have the entire book and you can read the entire book It s an infinite capacity book It s got a bunch of stuff in it And know that any time anyone queried the book the first time for a given x that there was this random bit number that was generated and put into the book And there s a whole lot of these numbers right So what s going to happen is you re going to have to look through the entire book this entire potentially infinite capacity book in order to figure out if this particular y is in the book or not And that s going to take a long time to do potentially OK So in the case where you have a random oracle you d have to go through and find looking at the output hash corresponding to each of the entries in the random oracle you re going to start matching match match match match it s going to take you exponential time Well actually worse than that given the infinite capacity of the book So this clearly gives you that Now you may not be a completely satisfied with that answer because you say well you can t implement that But we ll talk a little bit as I said about how you could actually get this But what s I should be clear is that the simple hash functions that we ve looked at in the past just to build dictionaries do not satisfy this right So suppose I had h of x equals x square mod p Is this one way given a public p No of course not right Because I m going to be it s going to be easy for me to do something Even though this is discrete arithmetic I could do something like well I know that what I have here actually let s do it with something that s simpler and then I ll talk about the x squared If I had something as simple as x mod p I mean that s trivially broken in terms of one wayness Because I know that h of x could be viewed as the remainder So anything if this is h of x and let s just call that y for a second because that s what we had it out there Something that s a multiple of y plus the remainder so I could have a is that right Is that what I want Yeah No plus y So I want a of well since I can t figure it out why can t you What do I need to put in there in order to discover an x that would produce a y Can I write an equation Yeah AUDIENCE Could you just write y itself SRINIVAS DEVADAS Just y itself That s right Good point Just y itself in this case Good I knew you guys were smarter than me This proves it So if you just take y and y remember is going to be something that s to p minus right And that s it It just goes through right So that s a trivial example right Now if I put x squared in here obviously it s not y but I could start looking at what I have here is I m going to get y that looks like x squared But I could take the y that I have take the square root of that and then start looking for x s that give me the y that I have Actually it s not a complicated process to try and figure out through trial and error potentially what an x is that produces a particular y for the kinds of hash functions that we ve looked at all right Now as you complicate this equation it gets harder Because you have to invert this set of equations And that s what the game is going to be when you go create one way hash functions The amount of computation that you do in order to compute the y is going to increase to the point where as I mentioned you have rounds of computation things getting mixed in And the hope is that you create this circuit if you will that has all this computation in that Going forwards is easy because you ve specified the multiplications and the mods and so on and so forth But not all of these operations have simple inverses And going backwards which is what you need to do in order to break one wayness or discover the x given a y is going to be harder and harder as the computations get more complex OK So everyone have a sense of what one wayness is So that s one wayness There s four other properties two of which are very related CR and TCR So CR is collision resistance It s infeasible to find x and x prime such that x not equal to x prime and h of x equals h of x prime which is of course a collision OK And that just says you have this crazy hash function where you can t discover collisions Well it would be absolutely wonderful In fact that s what we wanted when we built dictionaries But why don t we use SHA in dictionaries Why don t we use SHA in dictionaries Yeah AUDIENCE Because it s more complicated than we need SRINIVAS DEVADAS Yeah it s horribly slow right It would take longer to compute the hash than access the dictionary when you actually had a reasonable dictionary that maybe had some collisions I mean you just go off and you have a linked list you can afford a few collisions what s the big deal right So it just doesn t make any sense to use this level of heavyweight hash function even if it satisfies collision resistance which some of these are conjectured to do for the applications we ve looked at But there ll be other apps where collision resistance is going to be important So that s collision resistance And then there s TCR is target collision resistance It s a weaker form so sometimes people CR strong collision resistance and TCR weak occlusion resistance We ll use CR and TCR here And this says it s infeasible given x so there s a specific x that you want to find a collision for as opposed to just finding a pair that goes once to x and x prime And any pair would suffice to break the collision resistance property But TCR says is I m going to give you a specific x And I want you to find an x prime who s hash collides with the hash of x OK That s TCR OK that s TCR for you And that just to be clear I think you probably all got this obviously we want this here because we have a deterministic hash function And it s a trivial thing to say that if you had x and you had x again that you get the same hash back from it That s a requirement really So we want two distinct x and x primes that are not equal that end up colliding That s really what a collision is And so you see the difference between CR and TCR Yup Yeah AUDIENCE Are we to assume that given an x it s very easy to get the h of x back SRINIVAS DEVADAS So the question was given an x it s poly time computation to get h of x Absolutely Public poly time computation given an x to get h of x So going this way is easy Going this way I ran out of room hard OK AUDIENCE So does that mean that TCR is basically the same as SRINIVAS DEVADAS No no no absolutely not TCR says it s OK You can compute this You can get x And you can get h of x So given x you know that you can get h of x I didn t actually put that in the definition And maybe I should have So given x you can always get h of x Remember that It s easy to get h of x So any time I say given x you can always add it saying given x and h of x So I m given x I m given h of x I obviously need to map I need to discover an x prime such that h of x prime equals h of x OK Now you have situations where for it may be the case that for particular x s you can actually do this And that s enough to break TCR So you have to have this strong property that you really don t want to find collisions are for some even if there s a constant fraction of x s that break the TCR property you don t like your hash function OK Because you might end up picking those and go build security applications using those properties I want to talk a little bit about the relationship between OW CR and TCR So I m going to get back to that And we re going to talking about hash functions that satisfy one property but don t satisfy the other And I think your question will probably be answered better OK Thanks for the question So those are the main ones And really quickly if you want to spend a lot of time on this but I do want to put up I think I ll leave these properties up here for the duration Because it s important for you to see these definitions as we look at the applications where we require these properties or a subset of these properties But that we have pseudo randomness And this is simply a function of the fact that so this is PRF we know we can t build a random oracle And so we re going to have to do something that s pseudo random And basically what we re saying here is the behavior is indistinguishable from random So we re going to have to use non linearity things that are called non linear feedback shift registers to create pseudo random functions There s many ways that we can create pseudo random functions We won t really get into that But obviously that s what we want And then the last one is a bit tricky And we will have an app that requires this way at the end But this is infeasible given h of x to produce h of x prime where x and x prime are and it gets a little bit fuzzy here are related in some fashion right So a concrete example of this is let s say that x prime is x plus So this is a reasonable example of this So what this says is you re just given h of x It doesn t actually say anything about one wayness yet But you could assume for example that if this was a one way hash function that it would be possible to get x from h of x correct And let s keep that though Hold that thought all right We re going to get back to it So if I m just given the hash through some computation it may be possible for me to create another hash h of x prime such that there s some relationship that I can prove or argue for between the strings that created the hashes namely x and x prime OK That s what malleability is right Now you might just go off and say here s an x here s a y here s h of x and here s h of y These look completely random And you might go off I m being facetious here I say that y is x s third cousin s roommate s brother in law or something right I mean just make something up right So clearly there s got to be a strong precise relationship between x and y If in fact you could do this and get y equals x plus that d be a problem right But if you are and then you can do this sort of consistently for different x s and y s that would absolutely be a problem right But what you re really asking for and typically when you want non malleability it s things where you have auctions for example where you are to be careful about making sure that you don t want to expose your bid And so maybe what you re doing is exposing h of x You don t want somebody to look at your h of x and figure out how they could beat your bid by just a little bit Or in case of Vickrey auctions where the second highest bidder wins now just be a little bit below you right So that s the kind of thing that you want to think about when it comes to non malleability or malleability where you want a strong relationship between two strings that are related in some ordered fashion like x equals x prime equals x plus or just x prime equals times x And you don t want to be able to you don t want the adversary to be able to discover these new strings Because that would be the system all right So any questions about properties Are we all good on these properties All right because I m going to start asking you how to use them for particular applications or what properties are required for certain applications OK One last thing before we get there I promised a slightly more detailed analysis of the relationships between these properties So let s do that Now if your just look at it eyeball it and you look at collision resistance and TCR what can I say about the relationship between CR and TCR If h is CR it s going to be TCR right It s got to be TCR It s a strictly stronger requirement But not reverse And you can actually give a concrete example of a particular hash function that is TCR I m not going to go there It s actually a little more involved than you might think it is where a TCR hash function is not collision resistant But you can see that examples such as these should exist simply because I have a more stringent property corresponding to collision resistance as opposed to TCR right So if you re interested in that particular example you re not responsible for it get in touch with me and I ll point you to a like a three page description of an example So I didn t really want to go in there But what I do want to do is talk about one wayness and collision resistance Because I think that s actually much more interesting all right So if h is one way any conjectures as to what the question mark is in the middle Can I make strong statements about the collision resistance of a hash function if I m guaranteed that the hash function I have is a one way hash function or vice versa Another way of putting it is can you give me an example of just to start with a hash function which is one way but not TCR not target collision resistant So I m going to try and extract this out of you This is somewhat subtle But the way you want to think about this is let s say that h of x is OW and TCR OK And so I have a bunch of inputs And this is the output And I get d bits out And I ve got x x to xn OK Now I ve given this h I ve been given this h which is one way and TCR It satisfies those properties that you have up there In the case of one way I give you an arbitrary d bit string You can t go backwards and find a bunch of the xi s that produce exactly that d bit string all right So it s going to be hard to get here But you re allowed now to give me an example So this is some hash function that you can create which may use h as well And h is kind of nice because it has this one way property So let s say that we want to discover something where one way does not imply TCR So I want to cook up a hash function h prime such that h prime is one way but it s not TCR OK The way you want to think about this is you want to add to h And you want to add something to h such that it s still hard if you add h it s still hard to go from here to there Because you ve got to go deeper If you add to for example the inputs of h Or you could add to the outputs of h as well or the outputs of the current h But you can basically go deeper or need to go deeper in order to find the break one wayness in order to find an x whatever you have that produces the d bit string that you have right So what s a simple way of creating an h prime such that it s going to be pretty easy to find targeted collisions even not necessarily collisions it s pretty easy to find targeted collisions without breaking the one way property of h Yeah AUDIENCE So if you have x sub i if i odd then return h of x of i So that s minus So return the even group SRINIVAS DEVADAS Sure Yep AUDIENCE Given x any x of i you can usually find another x of i that was the same output You can go backwards SRINIVAS DEVADAS You can t go backwards Yeah that s good That s good I m going to do something that s almost exactly what you said But I m going to draw it pictorially And what you can do you can do a parity like odd and even that was just described And all I ll do is add a little [ XNOR gate which is a parity gate to one of the inputs So you have and b here So I ve taken x and I have a and b here So I ve added I can add as many inputs as I want to this function Oh I should mention by the way h of x is working on arbitrary strings And obviously I put in some number here that corresponds to n which is a fixed number So you might ask what the heck happened here with respect to arbitrary strings And there s two answers The first answer is well ignore arbitrary And assume that you only have n bit strings And n this is really large number right And that may not be particularly satisfying The other answer is which is more practical which is what s used in practice is that typically what happens is you do have particular implementations of hash functions that obviously need to have fixed inputs n for example And n is typically It s usually the block size And you chunk the input up into five bit blocks And typically what you do is you take the first five bits compute the hash for it And then you can do it for the remaining blocks And then you can hash all of them together all right So there s typically more invocations I don t really want to get into it But there s typically more invocations of h when the input would be times n or times n all right So we don t really need to go there for the purposes of this lecture But keep that in mind So we ll still stick with our arbitrary string requirement So having said that take a look at this picture And see what this picture implies I have an h prime that I ve constructed right Now if I look at h prime and I give you an output for h prime so h prime now has it s a function of a and b and x all the way to xn right So it s got an extra input If I look at h prime and I look at the output of h prime that is given to me and I need to discover something that produces that it is pretty clear that I need to figure out what these values are all right And I need to know what the parity of a and b is And maybe I don t need to know exactly what a and b are but I absolutely need to know what the parity of a and b are because that s x And the one way I d break would require me to tell you what the value of x is and the value of x and so on and so forth So it s pretty clear that h prime is one way right Everybody buy that h prime is one way But you know what I ve got target collisions galore right All I have to do is flip I have a equals and b equals And I have a equals and b equals They re going to give me the same hash right So trivial example but that gets to the essence of the difference between collision resistance and one wayness target collision resistance and one wayness all right So this is one way but not TCR simply because a equals b equals for arbitrary x s produce the same thing as a equals and b equals right So those are collisions So admittedly contrived But it s a counterexample Counterexamples can be contrived It s OK All right So that was what happens with that Let s look at one more interesting thing that corresponds to the other way right So what I want to show is that a TCR does not imply one wayness OK so now I want an example where it is clear that I have target collision resistance because I can just assume that And we re going to use the same strategy I m just going assume that I have an h that s target collision resistant And I m going to try and cook up an h prime that is not one way So I m going to assume that in fact h is TCR and OW And I m going to take away one of the properties And if I take it one of the properties I have a counterexample right So think about how you could do this You have h as before And I want to add some stuff around it such that it s going to be easy to discover for a large for a constant fraction of hashes that I ve given to me not for any old hash Because you can always claim that one wayness is broken by saying I have x I computed h of x now I know what given h of x I know what x is I mean you can t do that right So that s not breaking the one wayness of it It s when you have an h of x and this is the first time you ve seen it you re trying to find what x is right So how would you how would you set it up so you break the one wayness of h without necessarily breaking the target collision resistance of the overall hash function that you re creating And you have to do something with the outputs OK You have to do something This is a little more involved It s not as easy as this example It s a little more involved But any ideas Yeah go ahead AUDIENCE So x is less than b returns x If x is greater than b return [INAUDIBLE SRINIVAS DEVADAS Beautiful Right What color did you get last time AUDIENCE Blue SRINIVAS DEVADAS You got a blue last time All right Well you get a purple You have a set Actually we have these red ones that are precious that are no we don t We chose not to do red I don t know There was some subliminal message I think with throwing red Frisbees that we didn t like But OK So thank you And h of x is simply something where I m going to concatenate a zero to the x value and just put it out And clearly this is breaking one wayness because I m just taking the input I m adding a zero to it and shipping it out So it s going to be easy to go backwards right And this only happens if x is less than n as the gentleman just said Less than or equal to n in terms of the input length OK Otherwise I m going to do h of x So this is good news Because I m actually using the hash function in the case where I have a longer input string This is bad news for one wayness because I m just piping out the input And so if I get an x and I see what the x is out here and let s just say for argument s sake that you could even say that n is going to be something that is less than d which is the final output which has d bits And so if you see something that h prime produces that s less than d bits you instantly know that you can go backwards and discover what input produced that for the h prime right Because you just go off and you go backwards This is what it tells you Now on the other hand if it s larger obviously you can t do that But there s a whole lot of combinations that you can do that for So this breaks one wayness OK Now you think about TCR And what you want a show of course is that this maintains TCR So that s the last thing that we have to show We know that it breaks one wayness But if it broke TCR we don t quite have our example So we want to show that it actually maintains TCR which is kind of a weakish property that we need to maintain And the reason this maintains TCR is that there s really only two cases here obviously corresponding to the if statement And it s pretty clear that if x is less than or equal to n clearly different x s produce different h prime x s correct Because I m just passing along the x out to the output So if x is less than n I am going to get different hashes at the output I m just passing them out So that s easy And for the other case well I assume that h of x was CCR correct Because that was the original assumption that I had h which was CCR So in both cases TCR is maintained because else h of x maintains TCR all right So again a bit of a contrived example to show you the difference between these different properties so you know not to mix them up You know what you want to ask for what is required when you actually implement an application that depends on particular properties All right Any questions so far about properties or any of these examples We re going to dive in to using them OK So start thinking computer security Start thinking hackers protecting yourself against the bad guys that are out there who are trying to discover your passwords trying to corrupt your files generally make your life miserable And we ll start out with fairly simple examples where the properties are somewhat obvious and graduate to this auction bidding example which should be sort of the culmination of at least this part of the lecture And depending on how much time I have I ll tell you a little bit about how to implement hash functions But I think these things are more important from a standpoint of giving you a sense of cryptographic hashes All right Password storage How many of you write your password in an unencrypted text file and store it in a readable location There you go man Thank you for being honest And I do worse Not only do I do that I use my first daughter s name for four passwords I won t tell you what the name is So that s something that we d like to fix right So what do real systems do Real systems cannot protect against me using my first daughter s name as a password right So there s no way you can protect against that But if I had a reasonable password which had reasonable entropy in it so let s assume here that we have reasonable entropy in the password And you can just say bits And it s not a lot right bits is characters OK And you don t have to answer this how many of you have characters in your password Oh I m impressed OK So you ve got bits of entropy But the rest of you forget it This is not going to help you OK But what I want assuming you have significant entropy in your password because otherwise if there s not enough entropy you can just enumerate all possible passwords of eight letters And it s not that much It s raised to what have you And you can just go off And none of these properties matter You just you have your h of x It s public We ll talk about how we use that in a second But clearly if the domain is small you can just enumerate the domain So keep that in mind I talked about h of x and it s obviously going to be relevant here But suppose I wanted to build a system and this is how systems are built ETC slash password file assuming you have long passwords it does it this way otherwise it needs something that s called a salt But that s and we won t go there So we just assume a large entropy What is it that a system can do What can it store in order to let you in and only let you in when you type your password and not let some bogus password into the system Or somebody with a bogus password into the system Yeah go ahead AUDIENCE If you capture the password when you enter it and compare it to what s stored SRINIVAS DEVADAS Yes AUDIENCE If it s a one way hash you know you have what the correct password is SRINIVAS DEVADAS That s exactly right That s exactly right So it s a really simple idea a very powerful idea It as I said assumed that the entropy and I m belaboring the obvious now but it is important when you talk about security to state your assumptions But you do not store password on your computer And you store the hash of the password Now why do I store my password on the computer Because this is so inconvenient right So this is what the system does for me But the fact of the matter is if I lose my password this doesn t help me Because what the system wants you to do is choose a password that is long enough and the h is one way So anybody who discovers h of PW that is publicly readable cannot discover PW all right That s what s cool about this How do you let the person log in Use h of PW to compare against h of PW prime which is what is entered where PW prime is the typed password And clearly what we need is the disclosure of h of PW should not reveal PW So we definitely need one wayness What about what about collision resistance Our target collision resistance Think practitioner now right Are we interested in this hash function being collision resistant What does that mean in this case Give me the context in this particular application Yeah go ahead AUDIENCE It means that someone entering a different password will have the same hash [INAUDIBLE SRINIVAS DEVADAS Exactly So it means that what you have is a situation where you do not reveal and so what might happen is that h of PW prime equals h of PW But h of PW equals h of PW prime But PW is not equal to PW prime What you have is a false positive Someone who didn t know your password but guessed right and this is a bit value and they guessed right is going to get it You don t particularly care of the probability of this occurrence It s really small Typically you re going to have systems that lock you out if you try tries that occurs one two wrong passwords right So really in systems you do not require you do want to build systems that have minimal properties with respect to the perimeters that are used So from a system building standpoint just require OW Don t go overboard Don t require collision resistance or TCR OK Let s do a slightly different example Also a bit of a warm up for what s coming next which is a file modification detector So for each file F I m going to store h of F And as securely So you assume that this means that h of F cannot be modified by anybody h of F itself And now we want to check if F is modified by re computing h of F Which could be this could be modified So this could actually be F prime You don t know that You have a file It s a gigabyte And somebody might have tampered with one of the bits in the file All you have is a d bit digest that corresponds to h of F that you stored in a secure location And you want to check to see by re computing h of F the file that is given to you and comparing it with what you ve stored the h of F that you ve stored And so what property do we need in order to pull this off Of hash functions What precisely do we need to pull this off What is the adversary trying to do And what is a successful break A successful break is if an adversary can modify the file and keep h of F the same right That would be a successful break right Yup Go ahead AUDIENCE TCR SRINIVAS DEVADAS TCR Yeah absolutely You need TCR So you want to modify the file So you re given that the file the adversary is given the file which is the input to the hash and is going to try and modify modify the file right So let s do a couple more And we re going to advance our requirements here a little bit So those two are basic properties I want to leave this up there We re going to do something that corresponds to digital signatures So digital signatures are this wonderful invention that came out of MIT in a computer science laboratory again Ron Rivest and collaborators which are a way of digitally signing a document using a secret key a private key But anybody who has access to a public key so it could be pretty much anybody could verify the authenticity of that signature right So that s what a digital signature is So we re going to talk about public cryptography on Thursday in terms of how you could build systems or encryption algorithms that are public key algorithms But here I ll just tell you what we want out of them Essentially what we have here in the case of signatures we actually want to talk about encryption here are there s two keys associated with a public key system Anybody and everybody in the system would have a public key that you can put on your website And you also have a secret key that s like your password that you don t want to write down you don t want to give away because that s effectively your identity And what digital signatures respond to are that you have two operations You have signing and verification So signing means that you create a signature sigma that is the sign using your private key your secret key off a message M So you re saying this is this message it came from me right That s what signing means You have this long message and you sign it at the bottom You re taking responsibility for the contents of that message And then verification is you have M sigma and a public key And this is simply going to output true or false And so the public key should not reveal any information about the secret key And that s the challenge of building PKI systems that we ll talk about in some detail next time But we don t need to think about that other than acknowledging it today So the public and private key are two distinct things neither one of which reveals anything about the other Think of them as completely distinct passwords But they happen to be mathematically related That s why this whole thing works And that mathematical relationship we ll look at in some detail on Thursday But having said that take a look at what this app is doing for us right This is a security application And I haven t quite gotten to hash functions yet But I ll get to it in just a minute But what I want to do is emphasize that there s two operations going on One of which is a signature which is a private signature in the sense that it s private to me if I m Alice Or private to Alice And you re using secret information on this public message M because that s going to be publicized And you re going to sign the public message And then anybody in the world who has access to Alice s public key is going to be able to say oh I m looking at the signature which is a bunch of bits I m looking at the message which is a whole lot of bits And I have this public key which is a bunch of bits And I m going to be able to tell for sure that either Alice signed this message or Alice did not sign this message And the assumption here is that Alice kept her private key secret And of course what I just wrote there that the public key does not reveal anything about the secret key OK So that s digital signatures for you in a nutshell And when you do MIT certificates you re using digital signatures a la Rivest Shamir Adleman the RSA algorithm So you re using this all the time when you click on links for example And what happens is M is typically really large I mean it could be a file right It could be a large file And you don t necessarily want to compute these operations on large files So for convenience what happens is you end up hashing the file And for large M it s easier to sign h of M And so replace the M s that you see here with h of M all right So now that we re given that we re going to be doing h of M in here think about what we wanted to accomplish with M right I told you what we wanted to accomplish with M There s a particular message I m Alice I m going to keep my secret key secret But I want to commit to signing this message M all right And I want to make sure that nobody can pretend to be me who doesn t know my secret key And nobody does So if I m going to be signing the hash of the message now it comes down to today s lecture I m signing the hash of the message h of M What property do I require of h in order for this whole thing to work out Yeah go ahead AUDIENCE Is it non malleability SRINIVAS DEVADAS Non malleability but even before that suppose absolutely but non malleability is kind of beyond one of these properties over on the right You re on the right track right So do you want to give me a different answer You can give me a different answer AUDIENCE Oh I m not sure SRINIVAS DEVADAS OK What Yeah back there AUDIENCE I think you wanted to one way because otherwise you could take that signature and find another message that you could credit SRINIVAS DEVADAS I can make M public I can make M M can be public And h of M is public So one wayness is not interesting for this example if M is public And we can assume that M eventually gets public Because that s the message I m signing right I can also put M out So I want the relationship I want you to focus on the relationship between h of M and M and tell me what would break this system And you re on the right track Yeah go ahead Or way back there Yeah sorry about that AUDIENCE TCR SRINIVAS DEVADAS TCR Why TCR AUDIENCE [INAUDIBLE SRINIVAS DEVADAS So I have M So what happens here I should write this out I m given as an adversary I have M and h of M It is bad if Alice signs h of M but Bob claims Alice signed M prime Because h of M equals h of M prime right That is bad So the M is public could you stand up M is given There s a specific M and a specific h of M in particular that has been exposed And h of M is what was used for the signature So you want to keep h of M the same It s a specific one So it s not collision resistance it s target collision resistance because that s given to you And you want to keep that the same But you want to claim that oh you promised me not right If you can do that you signed saying you want to pay not then you ve got a problem So your thing is very close It s just that it doesn t need to be a strong relationship between the or the I mean I give you a concrete example of that But it could be more it could be less Anything that is different from what you signed be it with the numerical relationship or not would cause a problem and break this scheme all right Are we good All right one last example the most interesting one And as I guessed I m probably not going to get to saying very much about how cache functions are implemented But maybe I ll spend a minute or two on it So let s do this example that has to do with commitments Commitment is important right You want to commit to doing things You want to keep your promises And in this case we have a legal requirement that you want to be able to make people honor their commitments and not weasel their way out of commitments right And we want to deal with this computationally And let s think about auctions So Alice has value x e g an auction bid Alice computes what we re going to call C of x which is a commitment of x and cements it right C of x C of x is let s assume that the auctioneer and perhaps other auctionees as well see C of x You have to submit it to somebody right So you can assume that that s exposed And what is going to happen is when bidding is over Alice is going to open so this is C of x can be thought of as sealing the bid So that s the commitment You re sealing the you re making a bid and you re sealing it in an envelope You ve committed to that That s obviously what happens in real life without cryptography but we want to do this with cryptography with hash functions And so now Alice opens C of x to reveal x So she has to prove that in fact x was her bid And that it matches what she sealed When you open it up think about it conceptually from a standpoint of what happens with paper and then we have to think about this computationally and what this implies right So again I ll do a little bit of set up And then we have start talking about the properties that we want for this particular application So there are a bunch of people who are doing bidding for this auction I don t I want to be the first I don t want to spend a lot of money But I want to win All of us are like that right If I know information about your bid that is obviously a tremendous advantage So clearly that can t happen right If I know one other person s bid I just do plus on that If I know everybody else s I just do plus on the maximum So clearly there s some secrecy that s required here correct So C of x is going to have to do two things It can t reveal x Because then even maybe the auctioneer is bad Or other people are looking at this And you can just assume that C of x is the C of x s are all public But I also need a constraint that s associated with C of x that corresponds to making sure Alice is honest correct So I need to make Alice commit to something right So what are the different properties of the hash function that if I use h of x here that I d want h to satisfy in order for this whole process to work like it s supposed to work with paper and envelopes Yeah go ahead AUDIENCE It has to be one way [INAUDIBLE SRINIVAS DEVADAS It has to be one way And explain to me so I want a description of it has to be one way because why AUDIENCE Because you want all the c x s to be hidden from all the other options SRINIVAS DEVADAS Right C of x should not reveal x all right All right That s good Do you have more It has to be collision resistant OK I guess A little bit more You re getting there What why is it collision resistant AUDIENCE Because you want to make sure that Alice when she makes a bid that she commits that bid If she s not going to resist it then she could bid and then find something else SRINIVAS DEVADAS That s exactly right So CR because Alice should not be able to open this in multiple ways right And in this case it s not TCR in the sense that Alice controls what her bids are And so she might find a pair of bids that collide correct She might realize that in this particular hash function you know and a billion dollars collide right And so she figures depending on what happens she s a billionaire let s assume She s going to open the right thing She s a billionaire but she doesn t necessarily want to spend the billion OK So that s that right But I want more Go ahead AUDIENCE You don t want it to be malleable Assuming that the auctioneer is not honest because you don t want to accept a bribe from someone and then change everyone else s bid to square root of whatever they bid SRINIVAS DEVADAS That s exactly right Or plus which is a great example right So there you go I ran out of Frisbees You can get one next time So yeah I don t need this anymore You re exactly right There s another it turns out it s even more subtle than what you just described And I think I might be able to point that out to you But let me just first describe this answer which gives us non malleability So the claim is that you also want non malleability in your hash function And the simple reason is given C of x and let s assume that this is public It s certainly public to the auctioneer and it could be public to the other bidders as well Because the notion of sealing is that you ve sealed it using C of x But people can see the outside of the envelope which is C of x So everyone can see C of x You still want this to work even though all other bidders can see C of x So given C of x should not be possible to produce C of x plus You don t know x is But if you can produce C of x plus you win all right And so that s the problem Now it turns out you now say OK am I done I want these three properties And I m done right There s a little subtlety here which these properties don t capture So that s why there s more here And I don t mean to titillate because I ll tell you what is missing here But let s say that I have a hash function that looks like this And this here is non malleable It is collision resistant And it s one way all right So h of x has all these wonderful properties all right I m creating an h prime x that looks like this which is a concatenation of h of x and giving away the most significant bit of x which is my bid right I m just giving that away right The problem here is that we haven t really made our properties broad enough to solve this particular application to the extent that there s contrived cases where these properties aren t enough OK And the reason is simple h prime x is arguably NM CR and OW And I won t go into to each of those arguments But you can think about it right If I m just giving you one bit there s others there s a couple of hundred others whatever it is that I have in the domain It s not going to be invertible h prime x is not going to be invertible if h of x is not invertible h prime x is not going to be breakable in terms of collision resistance if h of x is not breakable and so on and so forth But if I had a hash function like that is it a good hash function for my commitment application No obviously not Because if I publicize this hash function remember everything is public here with respect to h and h prime you are giving away the most significant that corresponds to your bid in this particular hash function right So you really need a little bit more than these for secrecy for true secrecy But in the context of this example I mean it s common sense that you would not use the hash function like that right So it s not that there s anything profound here It s just that I want to make sure that you understand the nuances of the properties that we re requiring We had all the requirements corresponding to the definitions of NM and CR and OW And you need a little bit more for this example where you have to say something perhaps informally like the bits of your auction are scrambled in the final hash output which most hash functions should do anyway and h of x will definitely do But you kind of unscrambled it by adding this little thing in here corresponding to the most significant thing all right So I ll stop with that Let me just say that the operation or sorry the work involved in creating hash functions that are poly time computable is research work People put up hash functions and they get broken like MD was put up in and then got broken SHA and so on and so forth And so I just encourage you to look up SHA and just take a quick scan and what the complexity of SHA is with respect to computing the hash given an arbitrary string all right I ll stick around for questions ",
        "tags": "hash functionrandom oracle modelcryptographycryptographic function",
        "title": "21. Cryptography: Hash Functions",
        "url": "https://www.youtube.com/watch?v=KqqOXndnvic"
    },
    "#doc2": {
        "description": "",
        "subtitles": " hello in this segment I m going to introduce the task of information retrieval in including in particular what s now the dominant form web search the task of information retrieval can be may be defined as follows that our goal is finding material which is usually documents of an unstructured nature usually text that satisfies an information need that s what the person is looking for information on from within large collections usually stored on computers so there s lots of mentions there are prototypical cases and other kinds of information retrieval do exist so they re things like music information retrieval where there are sounds not text documents but effectively what we re going to talk about here is the case where all of those usually clauses hold there are many scenarios for information retrieval the one that people think I ll first these days is almost invariably web search but there are many others so searching your email searching the contents of your laptop computer finding stuff in some companies knowledgebase doing legal information retrieval to find relevant cases for legal context or something like that it s always been the case that a large percentage of human knowledge and information is stored in the form of human language documents and yet there s also for a long time being something of a paradox there so this is just a kind of a not quite real graph to give a flavor of things don t really believe the numbers on the left hand side are exactly what they mean but what we re showing is that in the mid s it was already the case that if you looked at the volume of data that there was some data that was in structured forms by that I mean things like Malaysian all databases and spreadsheets but there was already vastly more data in companies organizations and around people s homes there was in unstructured form the form of human language text however despite that in the mid s structured data management and retrieval was the developed field and they re already large database companies we re in the field of unstructured data management there was very little there are a few teeny little companies that did various kinds of corporate document retrieval and things like that that situation just completely changed around the turn of the millennium so if we look today the situation is like this so the data volumes have gotten larger on both sides but in particular they ve gotten larger on the unstructured side the mess of outpouring of blogs tweets forums and all those other places that now store massive amounts of information but there s also then being a turnaround on the corporate side so now we have huge companies that are addressing the problems of unstructured information retrieval such as the major web search giant s so let s start and just say a little bit about what a the base what is the basic framework of doing information retrieval so we start off by assuming that we ve got a collection of documents out of which we re going to do retrieval and then at the moment we re going to assume that s just a static collection later on we ll deal with when documents get added to and deleted from that collection and how we can go out and find them in something like a web search scenario then our goal is to retrieve documents that are relevant to the users information need and helps the user complete a task let me go through that once more a little bit more slowly with this picture so at the start what we have is that the user has some task that they want to perform let s take a concrete example suppose what I want to do is get rid of the mice that are in my garage and I m the kind of person that doesn t really want to kill them with poison so that s why you use the tasks that I want to achieve and so to achieve that task I feel that I need more information and so this is the information need I want to know about getting rid of mice without killing them this information need is the thing with respect to which we assess information retrieval system but we can t just take someone s information need and stick it straight into a computer so what we have to do to be able to stick it into a computer is to translate the information need into something that goes into the search box and so that s what s happened here we now have a query and so here s my attempt at a query how trap mice alive so that s taken this information need and I ve made an attempt to realize it as a particular query and so then it s that query that goes into the search engine which leads which then interrogate sour document collection and leads to some results coming back and that may be the end of the day that sometimes if i m not satisfied with how my information retrieval session is working i might take the evidence i got from there and go back and come up with a better query so i might decide that alive is a bad word and put in something like without killing and see if that works any better okay um what can go wrong here well there are a couple of stages of interpretation here so first of all there was my initial task and I made some decisions as to what kind of information need I had um it could be that I got something wrong there so I could have misconceptions so maybe getting rid of mice the most important issue is not whether or not I kill them but whether I do it humanely but we re not going to deal with that issue so much while we re more interested in in here is the translation between the information need and the query and there are lots of ways in which that can go wrong in this formulation of the query so I might choose the wrong words to express the query I might make use of query search operators like inverted commas which might have a good or bad effect on how the query actually works and so those are choices of my query formulation which aren t altering my information need whatsoever that s important when we talk about how to evaluate information retrieval systems that s a topic will say more about later but let me just give you the first rough sense of this because we ll need it for everything that we do so whenever we make a query to an information retrieval system we re going to get some documents back and we want to know whether the results are good and the very basic way in which we re going to see whether they are good is as follows we re going to think of two measures which are complementary one of which is precision so precision is the fraction of the retrieve documents of the system that are relevant to the users information needs so that s whether when you look at the results it seems like one document in is relevant to the information need you have or whether seven documents in are that s assessing whether the mix of stuff you re getting back has a lot of it results in it the complementary mixture measure is the one of recall recall is measuring how much of the good information that s in the document collection is the system is succeeding and finding for you at the fraction of relevant docs in the collection that are retrieved well given more precise definitions and measurements for information retrieval later the one thing I want to express right now is that for these measures to make sense they have to be evaluated with respect to the users information need for certain kinds of queries such as the ones we ll look at in the following segments there are documents that get returned are deterministic in terms of what query is submitted so if we were just going to evaluate what gets returned with respect to the user s query then we necessarily say that the precision is a hundred percent but that s not what we do what we do is think about the users information need and that the precision of the results is assessed relative to that so in particular if we just look back for a moment if there s being a Miss formulation of the query by the user or just they didn t come up with a very good query that will be seen as lower lowering the precision of the return results okay this is only the very beginning of our looking at information retrieval but I hope you ve already got a sense of how we think of the task of information retrieval and roughly how we can see whether a search engine is doing a good or a bad job on it ",
        "tags": "",
        "title": "18   1   Introduction to Information Retrieval Stanford NLP Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=kKaGLGAQrmw"
    },
    "#doc20": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " let s now see how to generate these snippets or other summaries from single documents now here s an example of a snippet this is again coming from Google I ve asked a question was cast metal movable type invented in Korea and Google s given me three little snippets with the answer and you can see that it s bold faced cases where words in my query occurred in the snippet and you can see the use of dots in the snippets telling you that it s combining pieces from different pages different excuse me different places in the page so let s see how these kind of snippets and other kinds of summaries based on single documents are built we can think of any summarization algorithm as having three stages the first stage is content selection extracting the sentences that we need from the document so we have some document as input and we re going to need to extract sentences so we might segment our sentences off maybe we use sentences with periods full sentences maybe we use some kind of moving window so we ve extracted some kind of little pieces little sentences and now from the segmented set of sentences we want to pick the ones that are important so I ve marked those with little little black little black dots here we picked some set of extracted sentences in our next task information ordering is we re going to decide what order the sentences go in so we have some now ordered set of important sentences and finally we might do some modifications to the sentences perhaps we re going to simplify them or something else so that s sentence realization and the result of these four steps is our summary now the most basic summarization algorithm a one that comes up a lot really only uses one of these three steps the content selection step so in the simplest possible algorithm we don t worry about what ordering the sentences come in and we don t modify the sentences at all we simply segment our document into sentences or maybe you there windows we pick the important ones and we leave them in the same order they came in so we re going to use what we call document order with the original sentences so this is a very simple baseline for summarization in one most web a snippet generation algorithms certainly use but most commonly used algorithm for content selection dates back to the very earliest paper in the field from it s pretty exciting that these ideas came out so early and the intuition is really very simple choose sentences that have salient or informative words well what s that mean well you ve seen tf idf that s a way of picking words that are particularly frequent and then don t contain words that occur in all documents so that s one way we might define salience eor informative ti turns out in summarization we tend to use another approach the log likelihood ratio or sometimes called topic signature approach and um that differs from tf idf in two ways why don t we use a slightly different statistic for picking for weighting each of the words and second instead of keep picking all the words we ll choose only the words whose weight is above some threshold the very salient words now log likelihood ratio gives us a statistic called lambda I m not gonna go into details but um they re in some lovely papers and we re gonna choose all words for whose the value of log lambda is greater than in this cutoff of so that gives us a threshold for which we can pick words that are particularly salient by this statistic so we re going to weight every word and the weight of word I is going to be if the word is especially associated with that document meaning occurs more times in that document than in the background corpus by some threshold otherwise we re gonna give it a weight of and again I m for details about how to compute the log likelihood ratio and the intuition about the statistics and you can see this lovely TED Dunning paper or or the linen hobby paper that proposed using it for summarization now we want to modify this algorithm for dealing with query focus summarization again we re not interested so much in pure summarization in today s lecture but how to use summarization techniques for question answering so this is topics signature based topic signature meaning pick the words that are particularly associated with a with a document um content selection choosing the sentences where we ve got queries all right and so we re going to modify the algorithm very slightly we re going to choose words that are informative Eve by log likelihood ratio or words that happen to appear in the query so we re gonna wait every word in a document we re gonna give it a weight of if it meets the log likelihood threshold it passes the threshold of about we re going to give it a weight of also if that word happens to appear in the query or question in otherwise we re gonna give the word of weight of and these weights are very simple you can imagine learning more complex weights and some research has gone into coming up with very powerful ways to learn detailed weights but works pretty well it turns out and now we re just gonna weigh a sentence or perhaps it s a window we don t have actual sentences we re gonna weigh it by the weight of the words so we re just gonna sum over all the words in our sentence of the weight of the words and we re going to take the average now the content selection algorithm we just described is unsupervised we didn t you have any label training set of which of summaries to learn weights from or things like that so that s an alternative approach supervised content selection so now if we had a labeled training set where for each document we had a good summary and we had an alignment for every sentence in the summary we knew what sentence it came from in the document you get the matching sentences now we could extract all sorts of features we could extract the position of the sentence in the document four sentences are very likely to be good summary sentences how long it is we could have all the features we had before word informativeness and things like that we could have other kinds of features based on discourse information that we might have and we might associate every sentence with some vector of features and now we could just train a binary classifier should I put this sentence in the summary yes or no and it might learn weights for all these features and any other features we can come up with now the algorithm this sounds good but in practice it turns out to be very hard to get label training data at this time when people actually write abstracts for sentences they re not always the authors don t always use the exact words and phrases until they don t use entire sentences that come from the document so finding perfectly labeled abstracts with extracts from the document is hard it s hard to do the alignment because they don t pick entire sentences they may be in words or phrases or chunks it s hard to figure out where those words came from even when they did pick them from the document and it turns out surprisingly perhaps that the performance is simply not much better than unsupervised algorithms so in practice unsupervised content selection just using log likelihood ratio or other simple measures of how salient or informative award and hence the sentences are the most common method for content selection so we ve seen how to generate summaries from a single document and the baseline algorithm we picked is simply come up with a simple statistical way to find a sentence that is very informative by looking for informative words and we talked about log likelihood ratio as an important way of finding these sentences ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "22 - 2 - Generating Snippets-NLP-Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=dOr4NX4Z6-g"
    },
    "#doc21": {
        "description": "MIT 6.006 Introduction to Algorithms, Fall 2011 View the complete course: http://ocw.mit.edu/6-006F11 Instructor: Erik Demaine  License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu",
        "subtitles": "The following content is provided under a Creative Commons license Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free To make a donation or view additional materials from hundreds of MIT courses visit MIT OpenCourseWare at ocw mit edu PROFESSOR Today we are going to do computational complexity This is rather different from every other thing we ve seen in this class This class is basically about polynomial time algorithms and problems where we can solve your problem in polynomial time And today it s about when you can t do that Sometimes we can prove you can t do that Sometimes we re pretty sure you can t do that But it s all about negative results when your problems are really complex And there s a lot of fun topics here This is the topic of entire classes like We re just going to get a hour flavor of it So think of it as a high level intro But we re going to prove real theorems and do real things and you ll get a sense of how all this works So I m going to start out with three complexity classes P EXP and R How many people know what P is And it is Polynomial time More precisely it s the set of all problems you can solve in polynomial time This is what the class is all about Almost every problem we have seen in this class there s one exception is in P Does anyone know the exception It s a good puzzle for you Not NP What s next EXP How many people know what EXP is Or you can guess Any guesses Exponential These are all the problems you can solve in exponential time If you want to be formal about it in this case exponential means to the n to some constant So not just the n but also to the n squared to the n cubed Those are all considered exponential and a polynomial is considered in the class EXP Now basically almost every problem you can dream of you can solve in EXP Exponential time is so much time And this class has always been about taking things that are obviously in EXP and showing that they re actually in P So if you want to draw a picture you could say OK here s all the problems we can solve in polynomial time Here s all the problems we can solve in exponential time And there are problems out here These are different classes And we want to sort of bring things into here as much as possible I actually want to draw this picture in a different way which is as a horizontal line So an axis I m going to call this computational difficulty You could call it computational complexity but that s a bit of a loaded term that actually has formal meaning Difficulty is nice and vague So I can draw an abstract picture This is not a true diagram but it s a very good guideline of what s going on So we have I m going to draw I believe three notches No eventually four so let me give myself some room We have over here the easy problems are P Then we have these problems which are EXP We re going to fill in something in the middle And then this is something called R So you ve got P is everything here EXP is all the way out to here in some abstract view The next thing is R How many people know what R is This one I had to look up It s not usually given a name No one Teaching staff You guys know it These are all problems solvable in finite time R stands for finite R stands for recursive Recursive used to mean something completely different back in the s when people were thinking about what s computable what s not computable These are basically solvable problems computable problems Finite time is a reasonable requirement I think for all algorithms And that s R Now I ve drawn this arrow to keep going because there are problems out here It s kind of discouraging but there are problems that are unsolvable In fact most problems are unsolvable We re going to prove that It s actually really easy to prove Kind of depressing but true Let me start with some examples before we get to that proof So I m writing examples of some things we ve seen So here s an example of a problem we ve seen Negative weight cycle detection I give you a graph a weighted graph I want to know does it have any negative weight cycles What classes is this problem in P We know how to solve this in polynomial time in VE time using Bellman Ford VE time well that finds negative weight cycles reachable from s But I guess if you add a source that can reach anywhere zero weight then that ll tell you overall that it s in P It s also in EXP of course Everything in P is also in EXP Because if you can solve it in polynomial time you can solve it in exponential time This is at most exponential time At most polynomial Here s a problem we haven t seen But it s pretty cool N by n Chess So this is the problem I give you So we re in an by n board and I give you a whole bunch of pieces on the board and I want to know does White win from here I say it s White to move or Black to move and who s going to win form this position This problem can be solved in exponential time You can sort of play out all possible strategies and see who wins And it s not in P There s no polynomial time algorithm to play generalized Chess This sort of captures why Chess even at eight by eight Chess is hard because there s no general way to do it So there s no special way to do it probably Computational complexity is all about order of growth So we can t analyze eight by eight Chess but we can analyze n by n Chess And that gives us a flavor of why by is so difficult Go is also in EXP but not in P lots of games are in this category lot s of complicated games let s say And so this is a first example of a problem that we know we cannot solve in polynomial time Bad news I also talked about Tetris a little bit Unlike the Tetris training which we saw this is sort of realistic Tetris all the rules of Tetris The only catch is that I tell you all the pieces that are going to come in advance Because otherwise it s some random process and it s kind of hard to think about what s the best strategy But if I tell you what s going to come say it s a pseudo random generator and you know how it works You know all the pieces that will come I want to know can I survive from a given initial board mess and for a given sequence of pieces This can also be solved in exponential time Just try all the possibilities We don t know whether it s in P We re pretty sure it s not in P And by the end of today s lecture you ll understand why we think it s not in P But it s going to be somewhere in between here Tetris is actually right here But I haven t defined what right here is yet And then the next one is halting problem So halting problem is particularly cool as we ll see or interesting It s the problem of given a computer program Python whatever it doesn t really matter what language They re all the same in a theoretical sense does it ever halt Does it ever stop running return a result whatever This would be really handy you re writing some code and you ve run it for hours and you don t know is that because there s a bug and you ve got an infinite loop Or is it just because it s really slow So you d like to give it to some program checking program that says will this run forever or will it terminate That s the halting problem And this problem is not in R There is no correct algorithm for solving this problem There s no way to tell given an arbitrary program whether it will halt Now in some situations take the empty program I can tell that it halts Or I take some special simple class of programs I can tell whether they halt or determine that they don t halt But there s no algorithm that solves it for all programs in finite time In infinite time I can solve it Just run it Run the program Given finite time there s no way to solve this And so this is a little bit beyond what we can prove today It s not that hard to prove but it takes half an hour or something I want to get to other things But if you take they ll prove this What I want to show you instead is an easier result that almost every problem is not in R I need one term though which is decision problems All of these problems I set it up in a way that the answer is binary yes or no Is there a negative weight cycle Yes or no Does White win from this position in Chess Can you survive in Tetris And does this program halt For various reasons basically convenience the whole field of computational complexity focuses on decision problems And in fact so decision problems are ones where the answer is yes or no That s all Why Essentially because it doesn t matter If you take a problem you care about you can convert it into a decision problem We can see examples of that later Decision problems are basically as hard as optimization problems or whatever But let s focus on decision problems The answer is yes or no Claim that most of them are uncomputable And we can prove this pretty easily if you know a bit of set theory I guess On the one hand I have problems I want to solve These are decision problems And on the other hand I have algorithms or computer programs to solve them I m going to think of computer programs because more precise algorithms can be a little bit nebulous for thinking about pseudocode what s valid what s invalid But computer programs are very clear I give you some code You throw it into Python Either it works or it doesn t And it does something Runs for a while How can I think about the space of all possible programs Well programs are things you type into a computer in ASCII whatever In the end you can think of it as just as a binary string Somehow it gets encoded in binary Everything is reduced to binary in the end on a computer So this is a binary string Now you can also think of a binary string as representing a number in binary So you can also think of a program then as a natural number some number between and infinity And an integer So usually we represent this as math bold N That s just You can think of every program is ultimately reducing to an integer It s a big integer but hey It s an integer So that s the space of all programs Now I want to think about the space of all decision problems So how can I define a decision problem Well the natural way to think of a decision problem is as a function that maps inputs to yes or no Function from inputs to yes or no Or you can think of that as and So what s an input Well an input is a binary string So an input is a number a natural number Input is a binary string which we can think of as being in N So we ve got a function from N to So another way to represent one of these functions is as a table I could just write down all the answers So I ve got well the input could be the number And then maybe it s a Input could be could be and then maybe output is Then the input could be whatever So I could write the table of all answers This is another way to write down such a function What we have here is an infinite string of bits Each of them could be or It would be a different problem But they all exist Any infinite string of bits represents a decision problem They re the same thing So a decision problem is an infinite string of bits A program is a finite string of bits These are different things One way to see that they re different is put a decimal point here Now this infinite string of bits is a number a real number between and It s written in binary You may not be used to binary point This dot is not a decimal point It s a binary point But hey Any real number can be expressed by an infinite string of bits in this way any real number between and So a decision problem is basically something in R the set of all real numbers whereas a program is something in N the set of all integers And the thing is the number of real numbers is much much bigger than the number of integers In a formal sense we call this one uncountably infinite and this one is countably infinite I m not going to prove that here today You may have seen that proof It s pretty simple And that s bad news That means that there are way more problems than there are programs to solve them So this means almost every problem that we could conceive of is unsolvable by every program And this is pretty depressing the first time I saw it That s why we put it at the end of the class I think you get all existential I mean the thing is every program only solves one problem It takes some input and it s either going to output yes or no And if it s wrong on any of the inputs then it s wrong So it s going to give an answer Say it s a deterministic algorithm No random numbers or things Then there s just not enough programs to go around if each program only solves one problem This is the end of the proof Any questions about that Kind of weird Because yet somehow most of the problems that we think about are computable I don t know why that is But mathematically most problems that you could think of are uncomputable Question AUDIENCE [INAUDIBLE PROFESSOR Yeah It s something like the way that we describe problems is usually almost algorithmic anyway And so usually most problems we think of are in EXP And so they re definitely computable There s some metatheorem about how we think about problems not just programs So that s all I m going to say about R So out here we have halting problem and actually most problems You can think of this as an infinite line and then there s just this small portion which are things you can solve But we care about this portion because that s the interesting stuff That s what algorithms are about Out here kind of nothing happens So I want to talk about this notch which is NP I imagine you ve heard about NP It s pretty cool but also kind of confusing But it s actually very closely related to something we ve seen with dynamic programming which is guessing So I m going to give you a couple of definitions of NP not formal definition but high level definitions So just like P EXP and R it s a set of decision problems And it s going to look very similar to P NP does not stand for not a polynomial It stands for nondeterministic polynomial We ll get to nondeterministic in a moment The first line is the same It s all decision problems you can solve in polynomial time That sounds like P But then there s this extra line which is via a lucky algorithm Let me tell you at a high level what a lucky algorithm does is it can make guesses But unlike the way that we ve been making guesses with dynamic programming with dynamic programming we had to guess something We tried all the possibilities A lucky algorithm just needs to try one possibility because it s really lucky It always guesses the right choice It s like magic This is not a realistic model of computation but it is a model of computation called nondeterministic model And it s going to sound crazy because it is crazy but nonetheless it s actually really useful even though you could never really build this on a real computer The nondeterministic model is not a model of real computation It is a model of theoretical hypothetical computation It gets at the root at the core of what is possible to solve You ll see why in a little bit So in this model an algorithm it can compute stuff but in particular it makes guesses So should I do this or should I do this And it just says It doesn t flip a coin It s not random It just thinks it just makes a guess Well I don t know Let s go this way And then it comes another fork in the road It s like well I don t know I ll go this way That s the guessing You give it a list of choices and somehow a choice is determined by magic nondeterministic magic And then the fun part is I should say at the end the algorithm either says yes or no It gives you an output The guesses are guaranteed this is the magic part to lead to a yes answer if possible So if you imagine the space of executions of this program you start here and you make some guess and you don t know which way to go In dynamic programming we try all of them But this algorithm doesn t try all of them It s like a branching universe model of the universe So you make some choice and then you make some other choice and then you make some other choice All of these are guesses And some of these things will lead to yes Some of these things will lead to no And in this magical model if there s any yes out there you will follow a path to a yes If all of the answers are no then of course it doesn t matter what choices you make You will output no But if there s ever a yes magically these guesses find it This is the sense of lucky If you re trying to find a yes that s your goal in life then this corresponds to luck And NP is the class of all problems solvable in polynomial time by a really lucky algorithm Crazy I know Let s talk about Tetris Tetris I claim is in NP And we know how to solve it in exponential time Just try all the options But in fact I don t need to try all the options It would be enough just use this nondeterministic magic I could say well should I drop the piece here here here here here or here And should it be rotated like this or like this or like this or like this I don t know So I guess And I just place that piece I make another guess where to place the next piece Then I make another guess where to place the next piece I implement the rules of Tetris which is if there s a full line it clears I figure out where these things fall I can even think about should I rotate at the last second If I don t know I ll guess Any choice you have to make in playing Tetris you can just guess There s only polynomially many guesses you need to make So it s still polynomial time That s important It s not like we can do anything But we can make a polynomial number these magic guesses And then at the end I determine did I die or rather did I survive It s important actually It only works one way Did I survive Yes or no And that s easy to compute I just see did I ever go above the top row So what this model says is if there is any way to survive if there is any way to get a yes answer then my guesses will find it magically in this model Therefore Tetris is in NP If I had instead said did I die then what this algorithm would tell me is there any way to die which the answer s probably yes unless you re given a really trivial input So it s important you set up the yes versus no correctly But the Tetris decision problem can I survive is in NP The decision problem can I die should not be in NP But we don t know Another way to think about NP And you might find this intuitive because we ve been doing lots of guessing It s just a little crazy There s another way that s more intuitive to many people So if this doesn t make sense don t worry yet This is another way to phrase it Another way to think about NP which turns out to be equivalent is that don t think so much about algorithms for solving a problem just think about algorithms for checking the solution to a problem It s usually a lot easier to check your work than it is to solve a problem in the first place And NP is all about that issue So think of decision problems and think about if you have a solution so let s say in Tetris the solution is yes In fact I need to say this probably The more formal version is whenever the answer is yes you can prove it And you can check that proof in polynomial time This is the more formal this a little bit high level What does check mean Here s what check means Whenever an answer is yes you can write down a proof that the answer is yes And someone can come along and check that proof in polynomial time and be convinced that the answer is yes What does convinced mean It s not that hard Think of it is a two player game There s me trying to play Tetris and there s you trying to be convinced that I m really good at Tetris It seems a little one sided but it s a asymmetric game So you want to prove Tetris is I want to show Tetris is in NP Imagine I m this magical creature Actually it s kind of funny It reminds me of a story On the front of my office door you may have seen there s an email I received maybe years ago oh no I guess it can t be that long ago Must ve been about years ago when we proved that Tetris is NP complete And the email says Dear Sir or whatever I am NP complete We don t what NP complete means yet but it s a meaningless statement So it doesn t matter that you don t know what it means It might get funnier throughout the lecture today And he s like I can solve Tetris I m really good at playing Tetris I m really good at playing Minesweeper all these games that are thought to be intractable He gave me his records and so on It s like how can I apply my talent So I will translate what he meant to say was I am lucky And this is probably not true but he thought that he was lucky He wanted to convince me he was lucky So how could we do it Well I could give him a really hard Tetris problem And say can you survive these pieces And he says yes I can survive And how does he prove to me that he can survive Well he just plays it He shows me what to do So proof is sequence of moves that you make It s really easy to convince someone that you can survive a given level of Tetris You just show what the sequence of moves are And then I as a mere mortal polynomial time algorithm can check that that sequence works I just have to implement the rules of Tetris So in Tetris the rules are easy to implement Its the knowing what thing to do is hard But in NP knowing which way to go is easy In this version you don t even talk about how to find the solution It s just a matter of can you write down a solution that can be checked Can prove it This is not in polynomial time You get arbitrarily much time to prove it But then the check has to happen in polynomial time Kind of clear That s Tetris And every problem that you can solve in polynomial time you can also of course check it Because if you could solve it in polynomial time you could just solve it and then see did you get the same answer that I did So P is inside NP But the big question is does p equal NP And most people think no P does not equal NP most sane people So this is a big problem It s one of the famous Millennium Prize problems So in particular if you solved it you would get million and fame and probably other fortune You could do TV spots I think that s how people mostly make their money You could do a lot You would become the most famous computer scientist in the world if you prove this So a lot of people have tried Every year there s an attempt to prove either what everyone believes or most often people try to prove the reverse that they are equal I don t know why They should bet the other way So what does P does not equal NP mean It means that there are problems here that are in NP but not in P Think about what this means This is saying P are the problems that we can actually solve on a legitimate computer NP are problems that we can solve in this magical fairy computer where all of our dreams are granted You say oh I don t know which way to go It doesn t matter because the machine magically tells you which way to go If you re goal is to get to a yes So NP is a really powerful model of computation It s an insane model of computation No one in their right mind would consider it legitimate So obviously it s more powerful than P except we don t know how to prove it Very annoying Other phrasings of P does not equal NP is these are my phrasings I them up you can t engineer luck You can believe in luck if you want But it s not something that we can build out of a regular computer That s the meaning of this statement And so I think most people believe that Another phrasing would be that solving problems is harder than checking solutions A more formal version is that generating solutions or proofs of solutions can be harder than checking them Another phrasing is it s harder to generate a proof of a theorem than it is to check the proof of a theorem We all know checking the proof of a theorem should be easy if you write it precisely Just make sure each step follows from the previous ones Done But proving a theorem that s hard You need inspiration You need some clever idea That s guessing Inspiration equals luck equals guessing in this model And that s hard The only way we know is to try all the proofs See which of them work So what the heck What could we possibly say This is all kind of weird This would be the end of the lecture if you say OK well we don t know That s it But thankfully I kind of need this board I also want this one but I guess I ll go over here Fortunately this is not the end of the story And we can say a lot about things like Tetris See I drew Tetris not just in this regime We re pretty sure Tetris is between NP and P That it s in NP minus P So let me write that down Tetris is in NP minus P We don t know that because we don t know this could be the empty set What we do know is that if there s anything in NP minus P if they are different then if there s anything in NP minus P then Tetris is one of those things That s why I drew Tetris out there It is in a certain sense the hardest problem in NP Tetris Why Tetris Well it s not just Tetris There are a lot of problems right at that little notch But this is pretty interesting because while we can t figure this out most people believe this is true And so as long as you believe in that as long as you have faith then you can prove that Tetris is in NP minus P And so it s hard It s not in P in this case In particular not in P That s kind of cool How in the world do we prove something like this It s actually not that hard I mean it took us several months but that s just months whereas this thing has been around since I guess the s P versus NP Why is this true Because Tetris is NP hard What does NP hard mean This means as hard as every problem in NP I can t say harder than because it s non strict So it s at least as hard as every problem in NP And that s why I drew it at the far right It s sort of the hardest extreme of NP Among everything in NP you can possibly imagine Tetris is as hard as all of them And therefore if there s anything that s harder than P then Tetris is going to be harder than P because it s as far to the right as possible Either P equals NP in which case the picture is like this Here s P Here s NP Tetris is still at the right extreme here But it s less interesting because it s still in P Or the picture looks like this and NP is strictly bigger than P And then because Tetris is at the right extreme it s outside of P So we prove this in order to establish this claim Just to get some terminology what is this NP complete business Tetris is NP complete which means two things One is that it s NP hard And the other is that it s in NP So if you think of the intersection NP intersect NP hard that s NP complete Let me draw on the picture here what this means So I m going to draw it on the top This is NP hard Everything from here to the right is NP hard NP hard means it s at least as hard as everything in NP That means it might be at this line or it might be to the right But in the case of Tetris we know that it s in NP We proved that a couple of times And so we know that Tetris is also in this range And so if it s in this range and in this range it s got to be right here Completeness is nice If you prove something is something complete prove a problem is some complexity class complete then you know sort of exactly where it falls on this line NP complete means right here EXP complete means right here Turns out Chess is EXP complete EXP hard is anything from here over EXP is anything from here over this way Chess is right at that borderline It is the hardest problem in EXP And that s actually the only way we know to prove that it s not NP It s is pretty easy to show that EXP is bigger than P And Chess is the farthest to the right in EXP of any problem in EXP and so therefore it s not in P So whereas this one these two we re not sure are they equal This line we know is different from this one We don t know about these two though Does NP equal EXP Not as famous You won t get a million dollars but still a very big open question What else do I wanna say Tetris Chess EXP hard So these lines here this is NP complete And this is EXP complete So the last thing I want to talk about is reductions Reductions so how do you prove something like this What is as hard as even mean I haven t defined that But it s not hard to define In fact it s a concept we ve seen already Reductions are actually a way to design algorithms that we ve been using implicitly a lot You may have even heard this term A bunch of recitations have used the word reduction for graph reduction You have some problem you convert it into a graph problem then you just call the graph algorithm You re done That s reduction In general you have some problem A that you want to solve And you convert it into some other problem B that you already know how to solve It s a great tool because in this class you learn tons of algorithms for solving tons of problems Now someone gives you in your job or whatever or you think about some problem that you don t know how to solve the first thing you should do is can I convert it into something I know how to solve because then you re done Now it may not be the best way to solve it but at least it s a way to solve it Probably in polynomial time because we think of B as things you can solve in polynomial time Great So just convert problem A which you want to solve into some problem B that you know how to solve That s reduction Let me give you some examples that we ve already seen just to fit this into your mental map of the class It s kind of a funny one but it s a very simple one So how do you solve unweighted shortest paths In general Easy one Give you a graph with no weights on the edges and I want to the shortest path from s to t AUDIENCE BFS PROFESSOR BFS Linear time right Well that s if you re smart or if you feel like implementing BFS Suppose someone gave you Djikstra Said here look I ve got Djikstra code You don t have to do anything There s Djisktra code right there But Djikstra solves weighted shortest path I don t have any weights What do I do Set the weights to It s very easy but this is a reduction a simple example of reduction Not the smartest of reductions but it s a reduction So I can convert unweighted shortest paths into weighted shortest paths by adding weights of Done Adding weights of would not work But weights of OK Weights of also works Pick your favorite number but as long as you re consistent about it That s a reduction Here s some more interesting ones On the problems set problem set six there was this RenBook problem I Can Haz Moar Frendz That was the name of the problem And the goal was to solve to find paths that minimize the product of weights But what we ve covered in class is how to solve a problem when it s the sum of weights How do you do it In one word or less Logs Just take logs That converts products into sums Now you start to get the flavor This is a problem that you could take Djikstra or Bellman Ford and change all the relaxation steps and change it to work directly with products That would work but it s more work You have to prove that that s still correct It s annoying to think about And it s annoying to program It s not modular blah blah blah Whereas if you just do this reduction you can use exactly the code that you had before at the end So that s nice This is why reductions are really the most common algorithm design technique because you don t want to implement an algorithm for every single problem you have It would be nice if you could reuse some of those algorithms that you had before Reductions let you do that Another one which was on the quiz in the true false quiz two was converting longest path into shortest path We didn t phrase it as a reduction It was just can you solve longest path using Bellman Ford And the answer is yes You just negate all the weights And that converts a longest path problem into a shortest path problem Easy Also on the quiz maybe I don t need to write all of these down because they re a little bit weird problems We made them up There was the what was the duck tour called Bird tours Bird tours Aviation tours Whatever You want to visit a bunch of sites in some specified order The point in that problem is you could reduce it to a single shortest paths query And so if you already have shortest path code you don t have to think much You just do the graph application Done Then there s the leaky tank problem which is also a graph reduction problem You could represent all these extra weird things that were happening in your car by just changing the graph a little bit And it s a very powerful technique In this class we see it mostly in graph reductions But it could apply all over the place And while this is a powerful technique for coming up with new algorithms it s also a powerful technique for proving things like Tetris is NP hard So what we proved is that a problem called Partition can be reduced to Tetris What s Partition Partition is I give you n numbers I want to know can I divide them into triples each of the same sum So I have n numbers Divide them into n over groups of such that the sum of each of the s is equal Sounds like an easy enough problem But it s an NP complete problem And people knew that since one of the first papers I guess that was late s early s by Karp So Karp already proved this is standing on the shoulders of giants Karp proved Partition is NP complete so I don t need to think about that All I need to focus on is showing that Tetris is harder than Partition This is what I mean by harder Harder means so when I can reduce A to B we say the A B is at least as hard as A Why s that Because I can solve A by solving B I just apply this reduction and then solve B So if I had some good way to solve B it would turn into a good way to solve A Now Partition which is A here we re pretty sure there s no good algorithm for solving this Pretty sure it s not in P And so Tetris better not be P either because if Tetris were in P then we could just take our Partition reduce it to Tetris and then Partition would be in P In fact all of the NP complete problems you can reduce to each other And so to show that something is at that little position NP complete all you need to do is find some known NP complete problem and reduce it to your problem So reductions are super useful for getting positive results for making new algorithms but also for proving negative results showing that one problem is harder than another And if you already believe this is hard then you should believe this is hard I think that s all I really have time for I ll give you a couple more NP complete problems Kind of fun Traveling salesman problem you may have heard of Let s say you have a graph And you want to find out the shortest path that visits all the vertices not just one vertex That s NP complete We solved longest common subsequence for two strings but if I give you n strings that you need to find the longest common subsequence of that s NP complete Minesweeper Sudoku most puzzles that are interesting are NP complete SAT SAT is a I give you a Boolean formula like x or y AND NOT x something like that I want to know is there some setting of the variables that makes this thing come out true Is it possible to make this true That s NP complete complete This was actually the first problem that was shown NP complete There s this issue right If I m going to show everything s NP complete by reduction how the heck do I get started What s the first problem And this is the first problem You could sort of prove it by definition almost of NP here But I won t do that Three coloring a graph Shortest paths This is fun Shortest paths in a graph is hard But in the real world we live in a three dimensional geometric environment What if I want to find the shortest path from this point where I am to that point over on the ceiling or something And I can fly That s NP complete It s kind of weird Shortest paths in a two dimensional environment is polynomial It s a good thing that we are on ground because then we can model things by two dimensions We can model things by graphs But in D shortest paths is NP complete So all these things where a problem knapsack that s another one We ve already covered knapsack We saw a pseudo polynomial algorithm Turns out you can t do better than pseudo polynomial unless P equals NP because knapsack is NP complete So there you go Computational complexity in minutes ",
        "tags": "computational complexitycomplexity classesreductionscomputational difficultydecision problemNP-completeNP-hard",
        "title": "23. Computational Complexity",
        "url": "https://www.youtube.com/watch?v=moPtwq_cVH8"
    },
    "#doc22": {
        "description": "Likelihood and maximum likelihood estimation. Model selection with Akaike information criterion (AIC).",
        "subtitles": " this video introduces maximum likelihood estimation which is a method for fitting models to data and comparing those models to find the best one we ll cover this topic in a fairly qualitative way for more details there are many good references particularly the book by Burnham amp Anderson on model selection and multi model inference so if you really want to know more particular about the mathematical end of things I would definitely recommend that reference so to introduce the concept of likelihood let s first think about probability which you ve heard about before in this class so for probability we can assume that we know something about the underlying reality or the true model or the or the overall population for example we can then ask what is the chance of observing this particular data or sample given that known reality so for example if the true population of quartz or the true proportion of quartz is what is the chance or what s the probability of finding percent in one particular sample or likewise if grain size has a normal distribution with some known mean mu and some known standard deviation Sigma what is the chance or what is the probability of observing some set of data X likelihood is just the reverse process so for likelihood we want to ask given some observed data what is the chance that a given reality or a given model is true so thinking about this in the reverse way if we counted percent quartz grains in the sample what is the chance or what is the likelihood that the true proportion is or if we observe some data X what is what are the best normal distribution parameters mu for the mean and Sigma for the standard deviation to describe that distribution lets likelihood so maximum likelihood estimation has two goals first it determines the best model parameters the most likely reality that fits a given set of data so this is the likelihood part we want to find the most likely parameters the most likely reality or model for the given set of data and second it s typically used to compare multiple models to determine which of those is the most likely explanation or the best fit for the data to do that it maximizes something called the log likelihood function to estimate the model parameters and then uses concepts from information theory to compare the model fits so I ll discuss each of these steps in a bit more detail starting with the likelihood function so let s take an example let s say we have some data like this histogram here there asteroid sizes or something like that we maybe want to know if it s best fit by a statistical distribution a or a digital distribution B and so I m not going to cover distributions in this course but so there s a lot of them different ones apply in different circumstances so basically by fitting a particular distribution and knowing that a particular distribution is the best fit for this data we can learn something about the underlying processes okay so if we start by thinking about probability let s say we assume that we know the distribution we can describe the probability of observing the data so this function here is the way that you read these sort of things with this conditional probability is that it s the probability of observing the data x x and so forth given that they come from a distribution with parameters theta so the vertical bar between the X s and the theta basically means that the probability of observing the X s is conditional upon the theta being true so given the way the conditional probabilities work we can also treat this as the probability of observing x given the parameters multiplied by the probability of observing x conditional upon those parameters and so forth so in other words we get the product that capital PI symbol of the probability of observing each data point given the parameters but remember that likelihood is just the reverse of probability so to get our likelihood function we can define it this way so this equation here states the likelihood of the true parameters being some values theta given the observed data x is the same thing or as equal to the probability of observing the data x given some true parameter values so what we want to do is we want to find the most likely parameter values given the data but it turns out that maximizing the likelihood function is difficult so what we re going to do is we re going to maximize the log likelihood function instead and so by taking the logarithm we can get rid of the product symbol capital pi a big pi there and instead use a sum so we re basically just going to get the log likelihood so that log L so the likelihood of the parameters being true given the data is the same thing as the sum of the log probabilities of the data being true given the parameters so the graph here shows the log likelihood blue or lower values red or higher values for likelihood and it s determined at each point in the graph by calculating the probability of observing the data given that particular mean and standard deviation so we can say okay if the mean was one and the standard deviation is two or sorry point two what is the probability of observing the data and that s our likelihood for that most parameters being true and then we could say okay let s say the mean is one point one and the standard deviation is point two what s the probability of observing the data and that s our likelihood for that point so the black dot shows the mean and standard deviation that you would calculate from the actual data it s also the maximum likelihood estimation point of this likelihood function so the maximum likelihood parameter estimate is exactly what you would calculate if you just use the raw actual data so you might be wondering at this point like what is the point of all this like you know if we can just get this value from the data why do we need to go through this whole process about finding the likelihood function and maximizing it and so forth well the real strength of this method is the ability to compare multiple models to find the one that is most likely of those given the data right so the maximum likelihood estimate is basically what you would just calculate from the raw data anyways but to be able to compare them we need to use information theory methods which are fairly complicated so I ll describe them in general terms ok so there s something an information theory called the Colbeck lie blur information and it basically just quantifies the amount of information lost when some probability density function which we ll call G is used to approximate model F and f is also some probability distribution function this is the equation that they give for this model G is evaluated over some parameter space theta and then the function is integrated for continuous distributions at least over X so just to the details of this integral function aren t critical because it actually isn t going to be used more generally we actually really want to know which of our candidate models that we re looking at best describes the underlying truth what s often called full reality so f is actually full reality it s not a model and G is one of our Mossman comparing multiple models to see how well they approximate full reality and basically figure out which one loses the least information when you re when you re approximating reality so full reality is of course not knowable so in practice this equation is rewritten into a form so that full reality becomes a constant and drops out of the equation I always find that kind of an amusing phrasing because it s it s you know reality is removed as a constant and you get this equation but basically then by turn this into sort of a relative form the models can be chosen on the basis of their relative distance from full reality okay but the copic lie blur information turns out to be extremely difficult to calculate even in a relative sense for many different functions but Japanese statistician hirotsugu Aki K showed that you could actually estimate this KL information from the maximum law likelihood and so he created a value which he called an information criterion or AIC and which is now called the Aki key information criterion so AIC is based on the maximum log likelihood in blue there theta with a little hat on it means that this is the maximum likelihood estimation point that s the the best parameters for that particular model and he also incorporated the number of parameters so the number of variables that you re using to fit this model which he gave as K or in which is in red here and so in this formulation here smaller AIC values indicate a better fitting model because they have higher likelihood like let s multiply so bigger likelihood or bigger log likelihood gives you smaller AIC and the way this equation is written so the K term then is often said to penalize more complex models that is models that have more parameters so adding more parameters always increases the model fit and always gives you a higher log likelihood however more parameters also increase the uncertainty in the model prediction for example because each parameter must itself be estimated from the data and so therefore there s some error on each estimate and if you re making lots of estimates you end up with lots more error on the overall fit of the model so basically AIC is a trade off between bias so like is the model bias towards being lower or higher so bias is reduced when you have more parameters as the model should be a better fit so that s a trade off between bias and variance or uncertainty variance is increased with more parameters because there s more sources of uncertainty to fit more variables as you re doing your model so relative differences between the AIC values for a variety of different models can be used to determine the best model of those candidates that you re looking at but you can also convert the AIC values in something called a ke key weights and they basically indicate that or tional support for each model they ll sum up to one or to a hundred percent so in the example that I introduced at the beginning in sort of our test case here model a which is the log normal incidentally is the best model of the two it has an icky icky weight of so notice those two models sum up to so gets of the support as we ll call it and one gets so this gives you a relatively strong support for model a over model B but there s no definitive cutoff in this sort of approach here this is exploratory statistics and so you need to use your judgment to determine you know if this is really strong support for model a or maybe maybe it s not and you re not as confident that a is better than B ok so just to end with some warnings when you re performing model selection and using AIC so both the likelihood values the maximum likelihood values log L and the AIC values are only relevant when you re comparing them among models run on the same data so you can t compare AIC on one data set to AIC on a different data set it s just fundamentally not meaningful because the absolute AIC value is not important the reason for this ultimately is that that callback lie blur distance or information is measured in a relative sense and so because it s relative the actual AIC value is not comparable across different data sets also in this is quite important to to be aware that AIC is only telling you which of the models is the best candidate of the ones you used so if you perform model selection on four separate models it will happily tell you that one of those models is the best but that doesn t actually mean that that model is itself necessarily good it s just the best of the ones that you looked at and so if you happen to choose four models that are all bad models it ll tell you which one of those is the best bad model but it might not be good in an absolute sense and finally don t combine model selection with hypothesis testing so if you use model selection to figure out which sort of which komban two variables for example is the best explanation for your distribution you shouldn t then do hypothesis testing and get a p value because the significance that you would get will be inflated you re examining multiple bottles and choosing the best and that s the same thing as testing multiple hypotheses and reporting only the most significant and you should never do that so model selection and AIC our exploratory methods which are valid methods to use but you should never mix these exploratory with hypothesis testing approaches ",
        "tags": "Statistics",
        "title": "30: Maximum likelihood estimation",
        "url": "https://www.youtube.com/watch?v=2vh98ful3_M"
    },
    "#doc3": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hello in this segment I m going to introduce the task of information retrieval in including in particular what s now the dominant form web search the task of information retrieval can be may be defined as follows that our goal is finding material which is usually documents of an unstructured nature usually text that satisfies an information need that s what the person is looking for information on from within large collections usually stored on computers so there s lots of mentions there are prototypical cases and other kinds of information retrieval do exist so there are things like music information retrieval where there are sounds not text documents but effectively what we re going to talk about here is the case where all of those usually clauses hold there are many scenarios for information retrieval the one that people think of first these days is almost invariably web search but there are many others so searching your email searching the contents of your laptop computer finding stuff in some company s knowledgebase doing legal information retrieval to find relevant cases for legal context or something like that it s always been the case that a large percentage of human knowledge and information is stored in the form of human language documents and yet there s also for a long time being something of a paradox there so this is just a kind of a not quite real graph to give a flavor of things don t really believe the numbers on the left hand side are exactly what they mean but what we re showing is that in the mid s it was already the case that if you looked at the volume of data that there was some data that was in structured forms by that I mean things like the Latian all databases and spreadsheets but there was already vastly more data in companies organizations and around people s homes there was in unstructured form the form of human language text however despite that in the mid s structured data management and retrieval was a developed field and already large database companies we re in the field of unstructured data management there was very little there are a few teeny little companies that did various kinds of corporate document retrieval and things like that that situation just completely changed around the turn of the millennium so if we look today the situation is like this so the data volumes have gotten larger on both sides but in particular they ve gotten larger on the unstructured side the mess of outpouring of blogs tweets forums and all those other places that now store massive amounts of information but there s also then been a turnaround on the corporate side so now we have huge companies that are addressing the problems of unstructured information retrieval such as the major web search Giants so let s start and just say a little bit about what a the base what is the basic framework of doing information retrieval so we start off by assuming that we ve got a collection of documents out of which we re going to do retrieval and then at the moment we re going to assume that s just a static collection later on we ll deal with when documents get added to and deleted from that collection and how we can go out and find them in something like a web search scenario then our goal is to retrieve documents that are relevant to the users information need and helps the user complete a task let me go through that once more a little bit more slowly with this picture so at the start what we have is the user has some tasks that they want to perform let s take a concrete example suppose what I want to do is get rid of the mice that are in my garage and I m the kind of person that doesn t really want to kill them with poison so that s why you use the tasks that I want to achieve and so to achieve that task I feel that I need more information and so this is the information need I want to know about getting rid of mice without killing them this information need is the thing with respect to where we assess in information retrieval system but we can t just take someone s information need and stick it straight into a computer so what we have to do to be able to stick it into a computer is to translate the information need into something that goes into the search box and so that s what s happened here we now have a query and so here s my attempt at a query how trap mice alive so that s taken this information need and I ve made an attempt to realize it as a particular query and so then it s that query that goes into the search engine which leads which then interrogate sauer document collection and leads to some results coming back and that may be the end of the day that sometimes if I m not satisfied with how my information retrieval session is working I might take the evidence I got from there and go back and come up with a better query so I might decide that alive is a bad word and put in something like without killing and see if that works any better okay um what can go wrong here well there are a couple of stages of interpretation here so first of all there was my initial task and I made some decisions as to what kind of information need I had um it could be that I got something wrong there so I could have misconception so maybe getting rid of mice the most important issue is not whether or not I kill them but whether I do it humanely but we re not going to deal with that issue so much what we re more interested in in here is the translation between the information need and the query and there are lots of ways in which that can go wrong in this formulation of the query so I might choose the wrong words to express the query I might make use of query search operators like inverted commas which might have a good or a bad effect on how the query actually works and so those are choices in my query formulation which are altering my information need whatsoever that s important when we talk about how to evaluate information retrieval systems that s a topic we ll say more about later but let me just give you the first rough sense of this because we ll need it for everything that we do so whenever we make a query to an information retrieval system we re going to get some documents back and we want to know whether the results are good and the very basic way in which we re going to see whether they are good is as follows we re going to think of two measures which are complementary one of which is precision so precision is the fraction of the retrieved documents of the system that are relevant to the users information need so that s whether when you look at the results it seems like one document in ten is relevant to the information need you have or whether seven documents in ten are that s assessing whether the mix of stuff you re getting back has a lot of it results in it the complimentary mixture measure is the one of recall recall is measuring how much of the good information that s in the document collection is the system is succeeding and finding for you at the fraction of relevant Docs in the collection that are retrieved will give more precise definitions and measurements for information retrieval later and the one thing I want to express right now is that for these measures to make sense they have to be evaluated with respect to the users information need for certain kinds of queries such as the ones we ll look at in the following segments there are documents that get returned are deterministic in terms of what query is submitted so if we were just going to evaluate what gets returned with respect to the users query and we d necessarily say that the precision is but that s not what we do what we do is think about the users information need and that the precision of the results is assessed relative to that so in particular if we just look back for a moment if there s being a miss formulation of the query by the user or just they didn t come up with a very good query that will be seen as lower lowering the precision of the returned results okay this is only the very beginning of our looking at information retrieval but I hope you ve already got a sense of how we think of the task of information retrieval and roughly how we can see whether a search engine is doing a good or a bad job on it ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "18 - 1 - Introduction to Information Retrieval-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=5L1qemKyUKA"
    },
    "#doc4": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hello in this section I m going to introduce the important idea of a term document matrix but also I m going to explain why it isn t actually a practical data structure for an information retrieval system or takers our example doing information retrieval over the works of William Shakespeare so let s suppose we have this concrete question which plays of Shakespeare containing the words Brutus and Caesar but not Calpurnia well if you re starting from a very basic level of text searching commands the first way that you think about to solve this problem is by using searching through the text of the documents exhaustively what s known in the UNIX world as grating and so we could kind of first of all grep for plays that contain Brutus and Caesar and then if you know your grep command well you can give a flag for files that do not match and you could get out the ones that don t contain Calpurnia now these days for works of the size of William Shakespeare for this kind of query grepping is a perfectly satisfactory solution our destroyers and computers are sufficiently fast that you would use this method and it takes no time at all to find the answer but nevertheless that isn t a good answer for the full information retrieval problem it falls flat in a number of ways once your corpus becomes large and so that means something like everything on your hard disk or even more so the world wide web we can t afford to do a linear scan through all our documents every time we have a query then some parts but like the not part become less trivial to implement the just finding things but even more so than the not part will have more complex queries like finding uses of the word Romans near countryman and we can t do that with a grep command but even more than that the big thing that s happened in information retrieval is the idea of ranking finding the best documents to return for a query and that s something that we just can t get out of the linear scan model of things that match and we ll talk about all of these issues the way they re handled in modern information retrieval systems in later lectures but let s first go to this idea of a term document matrix so what we do in a term document matrix is that we have the rows of the matrix are our words or often they re also called in information retrieval the terms and then the columns of the matrix are our documents and what we re doing here is a very simple thing we re simply saying let s fill in every cell in this boolean matrix by whether the word appears in the play or not so Anthony appears in Antony and Cleopatra but Calpurnia does not appear at Antony and Cleopatra so this matrix represents the appearance of words and documents and if we have this matrix it s straightforward to then answer boolean queries such as our example before queries for documents that contain Brutus and Caesar but not Calpurnia let s just go through concretely how we do that so what we re going to do is we re going to take the vectors for the terms and the query and then we re going to put them together with boolean operations so first of all we can take out the row that is referring to Brutus it goes up here then we can take the row for Caesar and end it there and then finally we can take the row for Calpurnia compliment it and then stick it down here so Calpurnia only appears in julius caesar and so we ve complemented it to a vector where everything is apart from Julius Caesar and at that point we can just add those three vectors together and our answer is this one of one zero zero one zero zero so we ve been able to do information retrieval successfully and can tell that these this query is satisfied by the documents in Antony and Cleopatra and Hamlet and indeed we can then go off to the document collection and confirm that that is the case so here we are so in Antony and Cleopatra when Antony found Julius Caesar dead he cried almost roaring and he wept when at Philippi he found Brutus slain and similarly refined both words occurring in Hamlet okay so that suggests that we could do information retrieval simply by working with this term document matrix so an important thing to realize is that that doesn t really work once we go to sensible sized collections and so let s just go through that for a minute let s go through a sensible sized but still small collection so suppose that we have million documents and will often use n to refer to the number of documents each of which is a on average a thousand words long ok so what does that mean in terms of the size of our document collection and in terms of the size of our matrix so if we have an average of six bytes per word including spaces and punctuation the amount of data we re talking about here is gigabytes so that s a teeny fraction of one modern hard disk in your laptop but let us then suppose we try and work out how many distinct terms there are in our document collection and we need to know the number of distinct terms because that corresponds to the number of columns now matrix and let s suppose they re about half a million that would be a typical number for a million documents and so often refer to this number of different terms as M well what does that mean well what it means is that even with that size document collection we can t build this term document matrix because we ll have rows and a million columns and that s half a trillion zeros and ones it s already huge and probably bigger than we have space to store and the document collection gets bigger than a million documents things are just going to get worse but this is really important observation which is although the matrix here had half a trillion zeroes and ones in that that actually almost all of the entries are zero that the document has at most billion ones and it d be good for you guys to stop and think for a fraction of a second why is it that there at most billion ones and the answer to that is well if we have million documents and the average document is a thousand were as long as we said last time then the actual number of word tokens is only billion so even if we assume that every word and every document were different we could only have at most billion entries and most likely we have far less than that because we ll have common words like that our oven to occurring many many times in each document and so therefore the key observation is the matrix we re dealing with is very very very sparse and so the central question in the design of information retrieval data structures is taking advantage of that sparsity and coming up with a better data representation and the secret of doing that and having an efficient storage mechanism is we want to only record the positions that hold a one and not the positions that hold a zero ok so I hope that s given you an understanding of the term document matrix it s an important conceptual data structure that we keep on coming back to again and again when we talk about various kinds of algorithms we think about them in terms of that matrix as you ll see but when we actually come to doing storage and computer systems we can also see that we never actually want to store documents and their information retrieval representation in that form ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "18 - 2 - Term-Document Incidence Matrices -Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=ftdII-X5SM0"
    },
    "#doc5": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hello again in this segment we re going to talk about the inverted index and how it s constructed and inverted index is the key data structure that underlies all modern information retrieval systems from systems running on a single laptop to those running in the biggest commercial search engines an inverted index is a data structure that exploits this sparsity of the term document matrix that we talked about in the preceding segment and allows for very inefficient sorry and allows for very efficient retrieval it s essentially without peer as the data structure used in information retrieval systems so let me go through what s in an inverted index so for each term each word we must store a list of all the documents that contain the word let s identify each document by a doc ID which is just a document serial number so you can think of us starting with the first document called then etc and then the question is what data structure should we use I mean one idea might be to use fixed arrays like the vectors they re in the term document matrix but that s very inefficient because while for some words they ll appear in a lot of documents other words will appear in very few documents moreover there are perhaps other problems if we think about a dynamic index where some documents are added later on that then we have or documents are changed then we ll have difficult things in adjusting our vector sizes for these reasons one way or another we need to use variable size lists the store the documents in which a word occurs and in standard information retrieval terminology these lists are called postings lists postings lists traditionally we usually stored on disk but that may not be the case for now for big search engines and if your story puts things list on disk the right way to store them is as one continuous run of postings because that gives the most efficient method of then being able to load them off disk back into memory when you re interested in the post from a particular word in memory our postings list can be represented as a data structure like a linked list or variable length arrays with some obvious trade offs in the size versus the ease of insertion so the data structure we end up with run inferred index is like the one that I m showing here so we have the terms that are in any of our documents and then for each term we ve got then got a pointer to a postings list that is then giving the different documents which are described by their document ID in which it occurs okay so one occurrence of a word document pair is referred to as a posting and the sum of all of the postings lists are then referred to as the postings and so overall then we have the parts of on the left hand side we have the dictionary and then on the right hand side we have the postings and these and a property of the postings is that they re sorted by document ID and very soon now we ll explain why that s essential so these two data structures the dictionary and the postings have somewhat different statuses because in global size the dictionary is relatively small but it s normally essential that it s in memory whereas the postings are large but at least for something like a small scale enterprise search engine these will normally be stored on disk let me move now to how an inverted index is constructed so the starting off point is we have a bunch of documents we indexed each of those documents we ll think of as being a sequence of characters we ll assume that we ve already dealt with pets by someone else s software conversion from PDF and Microsoft Word documents and things like that so then we re going to go through first some pre processing steps so we need a tokenizer that turns the document into a sequence of word token which are the basic units of indexing but we often don t index exactly the words that contain in the document there might be various linguistic modules than some way modify the tokens to put them into some kind of canonical form so for instance here we re saying that for friends here it s being both lower cased and as being stemmed to remove the s plural ending ok so then it s those modified tokens which will be fed to the indexer which is the thing that builds the inverted index that I was just talking about so here s the inverted index and as this step here of the index so that is the main thing that I want to talk about but let me first just briefly mention those initial stages of text processing so in just a fraction more detail the things that happen in those initial stages is firstly tokenization x that s just how we decide to cut the character sequence into word tokens and there are various issues there there are punctuation that come up against words how to treat possessives hyphenated terms and all that kind of stuff that we can talk about in more detail then normalization is this issue that well certain things like USA with and without the dots you probably want to treat us the same term and map both the text and the query terms to the same form so they will match you might want to do other kinds of mapping such as stemming so the authorize and authorization are both being mapped to the same stem so that they straightforwardly match in a query and finally you may not want to index at all the most common words traditionally many search engines have left out very common words like that are an of from the indexing it s not clear that in the modern world when our amount of storage is so vast that that s such a good idea because there are queries that you might like to do such as for the song to be or not to be where you really need the stop words and it turns out that with modern indexes it s not that inefficient to store them okay now let s go through in detail how the indexer goes from the sequence of Pat s normalized tokens to building an inverted index so for this example we re assuming we have two documents doc one and doc two here so there are key sequence of steps that we go through so our input is that we have the sequence of tokens of the first document in the order that they come in the text and the sequence of tokens of the second document in the order in which they come in the text so the first step is that we do a sort and restore as the primary key by the terms putting them in alphabetical order so here we have this alphabetical list of terms and if we have the same term appearing in multiple documents we do a secondary sort by the document ID so the word Caesar appears twice once in document ID one and twice in document ID and we re sorting it secondarily by document ID and so that s a core and expensive indexing step once we ve got that for our what we then do is essentially a consolidation of what we found over here on the right so we take that here it is again and multiple entries in a single document are merged so that s the two instances of Caesar and there just treat it as one and then we also merge all instances of a particular term and so then we represent that as over here so we say we have the dictionary entry Caesar we record its total frequency in the collection I ll come back to that a bit later and then we build fraud the postings list which is the list of documents in which it occurs and straightforwardly because of a consequence of our sort in the previous step that this postings list is itself going to be sorted by the document ID so I m thinking about the size of an inverted index we can think for a minute about where do we pay in storage so we pay some amount for the list of terms and there counts but the number of terms will be relatively modest now example before hand there are terms we pay for a pointer to that identifies where the postings lists are but again that s of the order of things and then we pay for the actual postings list themselves and these postings lists are by far the biggest part but even then they re bounded by the size of the number of tokens in the collection so an example in our example before of the million documents a foot of average length a thousand words we re still less than billion items there and so storage is manageable so when we are actually building an efficient ir system implementation we think further about these questions we think about how can we make the indexes as efficient as possible for retrieval and how can we minimize the storage on both sides of this both on this side and this side in terms of various sorts of compression we re not going to get into the details of that now but what I hope you can start to see is that the inverted index gives an efficient basis on which to do retrieval operations and that s something that we ll talk about in more detail in the next segment but at any rate now you know the underlying data structure it s really not that complex that underlies all modern information retrieval systems ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "18 - 3 - The Inverted Index-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=pevQ2T9Gm0w"
    },
    "#doc6": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " in this segment we re going to keep on looking at the inverted index and see how it s an efficient data structure for doing query operations in an ir system and in particular we ll step through in detail how you can perform a common kind of query and an query for two terms so starting off now we ll look at the details of query processing and then we ll have a later segment we ll talk even in more detail about the kind of queries we can process so suppose we want to process a query so suppose our query is Brutus and Caesar well let me even do a simpler example than that suppose the very first kind of query one we want to look at is just a query for Brutus well how to do that is totally straightforward what we do is locate Brutus in the dictionary and then we return its postings list that we look up and say ok this is the set of documents where Brutus occurs and we don t need to do anything else but now let s go to that fraction more complicated case well then we go in to locate for Brutus and Caesar we re going to locate both the words in the dictionary look up their postings lists and what we d like to do is then work out what are the documents that contain both Brutus and Caesar and doing the putting them together is standardly referred to as merging the two postings lists now that term can actually be misleading because what we re doing for an and query is we re actually intersecting the two sets of documents to find the documents that in which both words occur um whereas merging suggests doing some kind of putting them together in a union operation but the term merge is used actually in both cases so the merge algorithm family refers to a family of algorithms where you can step through a pair of sorted lists and do various boolean operations on let s look in concrete detail how that happens okay so the way we do merge operation to do Brutus and Caesar is like this we start with a pointer which points at the head of both lists and what we re going to be wanting to do is then work out what s in the intersection so the way we do that is we ask are these two pointers pointing at the same and equal dot ID and the answer is no and so what we do is then advance the pointer that has the smaller duck ID so now our two pointers are like this and we say does is are the two pointers pointing at the same document ID and here the answer is yes so we put that into a result list and then if we ve done that we can then advance both pointers we now say are these pointers both pointing at the same dock ID no is the first list greater first the thing pointed to by the first list pointer greater than the thing pointed to by the second list pointer no so we advance the bottom point one then we say is the dock ID pointed out by the two pointers equal no and so again we advance the smaller line equal no advance the smaller line at this point there again both pointing at the same dock ad so we add that to our result set and we advance both pointers are they the same no what we do is advance the smaller one are they the same no we advance the smaller one same no advance the smaller one same no advance the smaller one saying no and at this point when we try and advance the smaller one one of our lists is exhausted and so then there can be no other items in the intersection and so we can stop and so this is our return document set documents amp contain both Brutus and Caesar so I hope we went through that careful enough that you can see that if the list lengths are x and y then this merge algorithm takes big oh x y time that it s linear in the sum of the lengths of the two postings lists and you should also have seen what s crucial to make this operation linear and what s crucial to making it linear is the fact that these postings lists were sorted in order of document ID precisely because of that we could do a linear scan through the two postings lists where if that hadn t been the case then it would have turned into an N squared algorithm okay here s the postings list intersection algorithm one more time as a real algorithm but hopefully you can see it s doing exactly the same as what I was by hand so we start here with the answer set is zero and then we re going to be doing this while loop while the postings lists are both not equal to nil because as soon as one s nil we can stop so that s the end operation ok so then at each step what we do is ask whether the to the document ID of the two pointers is the same if so we edit to our answer if not and sorry and if they are the same we can advance both pointers and if not we work out which dock ID is smaller and then we advance that pointer so either this one or this one and that was exactly what I was doing and then as soon as one of the document lists runs out we can return our answer set okay I hope that made sense and you now feel like you could write your own code to do the intersection of postings lists using the merge algorithm ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "18 - 4 - Query Processing with the Inverted Index-Stanford NLP-Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=6Md_ZGW-wbk"
    },
    "#doc7": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " in this segment I m going to introduce phrase queries which in practice have been the most important kind of extended boolean query and then examine how you need to extend our inverted index data structure to be able to handle phrase queries so very often we d like to query not only for individual words but also for multi word units many useful things that we want to query on a multi word unit so information retrieval as a multi word unit or many many other things like organization names Stanford University so what we want to do is have a mechanism where we can see say let s let s match these pair of words as a unit and the syntax that standardly used in modern web search engines is supported in inverted quotes like that and so if we have that kind of phrase as a query then what we re going to say is a document that says I went to university at Stanford doesn t match that phrase query so this notion of putting things in inverted commas has proven to be very easily understood by users something that information retrieval system designers generally lament is that when people design advanced search functionality for their systems that almost nobody uses it just a few people like me and other academics and people like that but regularly users just don t use it and really this notion of phrase querying is sort of the nearest thing to an exception that it is fairly widely understood than used but I ll point out that as well as overt phrase queries like this it s actually another interesting phenomenon is that many other queries are implicit phrase queries so someone will give us their query San Francisco or something like that and well really what they want to do is have it interpreted as a phrase but they just didn t either or no to put the inverted quotes around it so in modern web search engines and active area of research has been how to identify these implicit phrases and take advantage of knowing it s an implicit phrase to alter what documents get returned but we re going to leave that aside for the moment and we re just going to deal with these explicit phrases and work out how we can do a good job at matching them with our information retrieval system okay well first what should be clear is that no longer suffice as the store just a term and a postings list of documents for each term because if we have only that we can easily find documents that contain two terms but we have no idea whether the terms are adjacent to each other as a phrase in that document the only way we could do it is by exhaustive post processing of candidate documents to see if they actually did contain the phrase and that would be extremely slow because we d be back to linear scanning of documents one way to solve this problem is the idea of a by word index so for by word index what we do is we index every consecutive pair of terms as a phrase so for example if we have this text here friends Romans countrymen we generate the by words of adjacent words of friends and Romans and then we generate the by word of Romans countryman and for each of these by words is now a dictionary term and so what does that mean well that means that for each of these we d say that friends Romans occurs in let s say this is document and maybe it also occurs in documents and Romans countrymen occurs in document and it might occur in some other document well if we build that two word phrase queering is now immediate because if we want to find documents that contain friends Romans in them we can just look up this dictionary entry and then grab the postings list that was returned for it if we consider more cases we can even basically handle those so we can handle longer phrase queries by breaking them down so if our phrase query is Stanford University Palo Alto we can break it up into an and query of Stanford University and University Paulo and Palo Alto and so we can use our by word index to find documents that contain all three of these by grams now that s not going to be perfect without doing a linear scan through the docs we can t be sure that the documents that we re returning actually have this continuous phrase Stanford University Palo Alto but it seems highly likely that they will because we have checked for each of these by Graham s occurring so there s a small chance of false positives but it seems like we re in a pretty good state so one of the issues with using a byword index well as we noted before there can be false positives in what s returned but maybe that s not such a big problem the big problem is that there s this enormous blow up of the index because our dictionary has gotten much more massive so that means that you know it s not practical to have try word and quadword indices to actually exactly match long queries but even for by words we ve been going to have the sort of potentially have the space of words squared possible dictionary entries so because of that I word indices and not the standard solution for handling free searching but as I ll show it wasn t useless that I explain them to you because as I ll show at the end they can be an important component of a solution so what is the standard solution the standard solution is to move to having a positional index so the idea of the positional index is that in the postings we in store not only the documents that contain a term but also the position in each document where the term IP years so the organization of what we have now is in the dictionary we have a term and the document frequency of the term and then when we point off to the postings lists then instead of just having a list of document IDs we then have a list of document IDs where for each document we then have a list of positions where the term occurs let s look at that concretely with an example so here we have the word B which occurs in almost a million documents and then we have where so in document one it occurs at word seven etc document two it occurs in these two word positions it doesn t occur in document document it occurs in a bunch of places and so on and so with these we can then be able to check when the Frasers occur by then seeing where the words occur adjacent to each other to get the idea of that you could consider this little question here so which of these four documents could contain to be or not to be based on those document positions now obviously we haven t seen the postings list for the other words but just looking at where B occurs which document is candidate so with this kind of data structure we can handle phrase queries using a merge algorithm much as we showed before it s just a little bit hairier because rather than just doing an intersection of the document IDs we have to do this sort of two level algorithm where we both intersect the document IDs and then also check that there are compatible positions for the words occurring in a phrase and so that means that we need to deal with a bit more than just equality so for example if we re wanting to find instances of the phrase information retrieval what we want is that if the word information occurs at position in a certain document you want to tree vault retrieval appearing at position in the document so going through them slightly more detail to process a phrase query what we do is so let s assume our phrase is to be or not to be that we want to find in inverted commas so we find we get the postings lists of each of the individual terms and then what we re going to do is we re going to progressively intersect them so if we start off with the to B we re going to start at the beginning doing postings merge and we re saying well document can t be a candidate because it doesn t appear on the other postings list document can t be a candidate because it doesn t appear on the other postings list and then at this point we ve got to document and the doc IDs match but then at this point with a positional query we have to recurse and then do another merge algorithm for the positions within the postings list so we start here and here but this time rather than a quality check what we wanting to see is can we find a place where B occurs with a token number larger than so we ll again step along and we ll say well here s one candidate and and here s a second candidate and so those will be candidate matches inside document and so we ll be returning both of those candidate matches separately because we re actually finding the positions in documents where our phrase query is matching and that we ll need to refine with the other query words coming up ahead at that point we then revert back oh wait no sorry I m wrong there s one more candidate sorry there are three candidates here right but once we ve exhausted those the positions and one document we then revert up to the higher level of our postings merge and then we say five is less and seven so this gets crossed off and then we advance that pointer and proceed along and I hope you can see that actually this method works not only for phrase queries where the words are offset by one that you can actually use exactly the same method for the proximity queries that we saw earlier with the Westlaw service where you could ask for one word being within three words of another word or something like that right so that was the examples like this so here we had limit within three words of statute which is within three words of federal within two words of tort so we could use the same kind of techniques of optimizing the query but if we started with this one within having a condition on how close together the token position indices have to be to count as a match within a document and so clearly positional indices can be used for these kind of proximity queries whereby word indices do nothing for you so I ve sort of said the algorithm here but it s something that you might want to work through very concretely is how you can change the linear postings merge algorithms to handle proximity queries in particular you can think about how to get it to work for any value of K it s actually a little bit tricky to do this correctly and efficiently because you can have the width in keywords matching on either side of the word you have to keep track of things for the right number of words on each side if is be good to try and work out by yourself but you can also see one example of an answer in the bigger of our introduction to information retrieval book which you can find free on the web to look at so a positional index is great because it allows us to answer phrased queries and proximity queries but there s a cost to a positional index which is that our postings list just got a lot larger because rather than only storing document IDs they re storing document IDs and the offsets of tokens within the document and that s a major factor even though indices can be compressed nevertheless is now completely standard to use a positional index because of the power and flexibility you can get from handling phrase and proximity queries whether they re being used explicitly in terms of having these kind of phrase queries or within three queries or whether they re just being exploited to improve the ranking of a system when it s looking from plis of phrases but as I said the positional index gets much larger and so when we re estimating the size of a positional index we note that there has to be an entry for each occurrence of a word not just once a document so what that means is that the size of the index depends on the average length of the documents now so if we have fairly short documents it s not such a big deal because actually most of the words the cur in a document occur only once or twice but if we have very long documents then that blows out the size of positional index rather more so for example you can consider a word with a common word with frequency so this word occurs one word in a thousand on average and so well if you have a document size of average length then we can again no blow out really from going to a positional index because they ll only be one position and being recorded but if we have a document the documents that are really long then we might be getting a hundred times blowout in the size of the positional index so everything that depends but you know just to give you some very rough rules of thumb for you know what is in some sense typical documents like webpages that you might expect as index to be somewhere around two to four times as large as a non positional index and in particular the size of a positional index remains smaller than but starts to approach the volume of the size of the original text so heading to a sort of a third or half the original size of the text that s being indexed which is much larger than in the case of a non positional index where it might be down to something like just so having an IR system doesn t actually take a lot more space than storing the text in the first place I mentioned when I mentioned by word indices that they are the useless idea even though they re not normally the solution by themselves and so let me just come back to that these two approaches can be very profitably combined so if you look at the query stream of high query volume services like web search engines there tend to be some queries that keep on turning up again and again and again so things like person names of popular people well if we just treat those as you know just regular postings listening to section or the positional phrase query posting list intersection what we d have to do is keep on doing this intersection over and over again every time that someone sends that question and it s bad for cases like Michael Jackson it s less bad when you ve got rare names like Britney Spears because presumably they re posting lists are much shorter and the intersection is roughly the same as each individual postings list this in doing this intersection every time is especially bad when you have common words like if you re searching for the band though the who that what s going to happen is you re going to retrieve two enormous postings lists do the intersection of them and end up with a very short postings list for this phrase query so in a paper from a group in Melbourne that s a well known information retrieval group in they investigated a more sophisticated mixed indexing system so in this what happened was that for common phrases you know like these examples they did build by words and index the by words where for rarer phrases they were done by a positional index and so what they were able to show was with a typical web query mixture you could execute it in one quarter of the time the positional index alone by making use of it also having indexed some by words but at the cost of only taking more space than having the positional index alone so that makes that it look a fairly appealing trade off to augment a positional index with common by grams and while one model of doing that is to do it as here where you work out in advance the common by grams and then index those in your standard inverted index in practice what happens a lot in modern system since if people try to do this a bit more dynamically so that they keep a cache of commonly being queried phrase queries and that what the result of the postings list intersection is for each of those and so that s like having added those by words to the inverted index but done a bit more dynamically okay so that s introduced what s the most extension to the classic boolean retrieval model which is having support for phrase queries and we introduced a method for handling those more two methods in particular we looked at positional indices which can handle phrase queries but also the proximity queries that we saw earlier ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "18 - 5 - Phrase Queries and Positional Indexes-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=pLeAMnmbh34"
    },
    "#doc8": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " hello in this segment I m going to start talking about vector retrieval so so far all of our queries have been boolean we give a query like ocean and liner and the search engine is going to return precisely the documents that contain the words ocean and liner and no others documents either match or they don t now this can be good for expert users with a precise understanding of their needs in the collection and it can also be good when information retrieval is a consistent in a larger application and the system can consume thousands of results and adjust the query as needed but such a system turns out not to actually be good for the vast majority of users most users are incapable of writing good boolean queries or even if they can write them they think it s far too much work to write them and in particular boolean systems often produce thousands of results and users don t want to wade through thousands of results this is particularly true of web search applications in general there s this problem with boolean search the feast or famine problem boolean queries often result in either too few return results one or two or even zero because documents don t precisely satisfy the search request or else they would result in too many results in the order of thousands of more so for example here s a system giving results if I give it the query standard user d link it returns results so I try and make my query more specific and ask standard user d link no card found but then I get zero results it takes a lot of skill to come up with a query that produces a manageable number of hits the basic problem is if you re putting an and between words you get too few results and if you put all between words you get too many results so part of the solution of that has been the development of ranked retrieval models and the idea of these is that rather than a set of documents satisfying a query in rate retrieval models the system returns an ordering over the top documents in the collection with respect to the query going along with that has been the adoption of free text queries rather than explicit query language like the boolean retrieval model with its ends or the not instead the users query is now just some words in a human language in principle these are two separate choices which could be manipulated separately but in practice mate retrieval models have normally been associated with free text queries and the opposite for the boolean retrieval model the fact a feast or famine problem doesn t exist in rate retrieval when a system produces a large result said users don t really notice indeed the size of the results set basically isn t an issue because normally the system will start off by just showing the use of the top view results and so not overwhelm the user and so the total number of results is something they probably might even know or notice this all depends on having a ranking algorithm that works well though so that the top results are good results so the basis of rate retrieval is having a good system of scoring we need to be able to return the documents that are most likely useful to the searcher and so that raises the question of how can we rank order the documents with respect to a query and the method we ll look at that to do that is the idea that what we should do is assign a score or say a number between and to each document the score met measures how well all the document in the query match each other so we need a way of assigning a score to a query document pair let s start with a one term query well if the query term doesn t occur in the document the score for the document should be zero and then beyond that probably what we want to say is that the more frequently the query term appears in the document the higher the score should be and after that how exactly do we score documents isn t quite so clear and so in upcoming segments we ll look at a number of alternatives for this ok but I hope that s given you an idea of what ranked retrieval models are and how they differ from the boolean retrieval model ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 1 - Introducing Ranked Retrieval-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=5Gz3Hp217Io"
    },
    "#doc9": {
        "description": "If you are interest on more free online course info, welcome to: http://opencourseonline.com/ Professor Dan Jurafsky & Chris Manning are offering a free online course on Natural Language Processing starting in March 19, 2012. http://www.nlp-class.org/ Offered by Coursera: https://www.coursera.org/",
        "subtitles": " as a first simple example of the ranked retrieval model let s consider scoring with the jacquard coefficient so the jacquard coefficient is a commonly used measure for the overlap between two sets a and B and what it is is simply you take the number of items in the intersection of a and B and you divide it by the number of items in the union of a and B and so if we take the jacquard coefficient of a set with itself then the set has some size and then that s size will be also the size of the intersection the Union and so the ratio be and the jacquard coefficient is if two sets are disjoint and have no members in common then the numerator of the jacquard coefficient will be and so that card coefficient is now the two sets don t have to be the same size but you should be able to see that if the jacquard coefficient will always assign a number between amp because that most the intersection can be as large as the union so suppose we decide to make the query document match score the Jaccard coefficient computed for the sets of words the two documents contain so the idea is that let s suppose our query is Ides of March which has these three words and then we have these two documents so what we can do is say so there are three different words here and then for document one Caesar doesn t occur in it die doesn t occur and then in doesn t occur in it March does occur in it so this size of the intersection is just one word and the total number of words is if we then do the second document well that doesn t occur in the query long doesn t occur in the query but again March does occur in the query so this time the jacquard coefficient of b comma d is going to be divided by the number of words which is this time five okay and so this document is going to win as having the higher jacquard score now of course that difference may not seem very significant essentially this document is winning here just because it s shorter but if we imagined a different example where we maybe had the word Ides in the second document then we d get that the jacquard coefficient for it is now two overlapping words over six and that maybe makes more sense to you that you re getting more overlap so the jacquard score is higher but this idea that all else being equal a shorter document should be preferred is a common one that we ll see again in ir models okay so is jacquard scoring a good idea for a retrieval model in general it s not felt to be it has a couple of issues one is that it doesn t consider term frequency it just uses the set of words in a document ignores how many times the words occur in a document but that s typically not all the information we want and we ll look at models in a minute the do deal with term frequency there s also a second finer point which is that rare terms in a collection and more informative and frequent terms when evaluating a query and that s something that we ll also want to factor into our models there s one other aspect in which the Jaccard coefficient turns out not to work very well and that is the way in which it does normalization by dividing through by the Union isn t necessarily quite right I mean in particular later on in these segments we ll introduce the idea of using cosine similarity and we ll go through the math of that for the more general case but if after you ve seen that you want to kind of come back to this and work out what the cosine similarity score is if you just have a bag of words and work out a cosine score it ll turn out that this which is like the Jaccard coefficient except that we ve got this slight difference in the denominator that we re now taking the square root of the union and that actually turns out to be a better form of length normalization okay and so I introduced the jacquard coefficient just as a very simple example of a ranked retrieval model but I think it was I hope it was also a way to show some of the issues that we need to deal with in a good retrieval model how to factor in the frequency of terms the rareness of words and how to normalize the score for different document lengths ",
        "tags": "NLPNatural Language ProcessingStanford UniversityStanford NLPDan JurafskyChristopher ManningOpenCourseOnlineCoursera",
        "title": "19 - 2 - Scoring with the Jaccard Coefficient-Stanford NLP-Professor Dan Jurafsky & Chris Manning",
        "url": "https://www.youtube.com/watch?v=Vbdki_gnnYM"
    }
}